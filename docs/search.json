[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Coriell Bioinformatics Notes",
    "section": "",
    "text": "Bioinformatics Notes\nThis book contains assorted bioinformatics related notes. It contains some workflows for RNA-seq, ATAC-seq, and methylation analysis, as well as some notes describing best practices for structuring data projects, and presenting science. We hope to keep this book updated as more useful topics come up in the lab.",
    "crumbs": [
      "Bioinformatics Notes"
    ]
  },
  {
    "objectID": "data-best-practices.html",
    "href": "data-best-practices.html",
    "title": "‘Best’ practices for data projects",
    "section": "",
    "text": "Quick note (TODO)\nScience requires reproducibility, not only to ensure that your results are generalizable but also to make your life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects and learning how to keep your data safe and easily searchable.\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution).",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#why-care-about-data-management",
    "href": "data-best-practices.html#why-care-about-data-management",
    "title": "‘Best’ practices for data projects",
    "section": "Why care about data management?",
    "text": "Why care about data management?\nComputing is now an essential part of research. This is outlined beautifully in the paper, “All biology is computational biology” by Florian Markowetz. Data is getting bigger and bigger and we need to be equipped with the tools for storing, manipulating, and communicating insights derived from it. However, most researchers are never taught good computational practices. Computational best practices are imperitive. Implementing best (or good enough) practices can improve reproducibility, ensure correctness, and increase efficiency.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-types-and-file-names",
    "href": "data-best-practices.html#file-types-and-file-names",
    "title": "‘Best’ practices for data projects",
    "section": "File types and file names",
    "text": "File types and file names\nAs a data scientist you’ll be dealing with a lot of files but have you ever considered what a file is? Files come in all shapes and formats. Some are very application specific and require specialized programs to open. For example, consider DICOM files that are used to store and manipulate radiology data. Luckily, in bioinformatics we tend to deal mainly with simple, plain text files, most often. Plain text files are typically designed to be both human and machine-readable. If you have the choice of saving any data, you should know that some formats will make your life easier. Certain file formats like TXT, CSV, TSV, JSON, and YAML are standard plain text file formats that are easy to share and easy to open and manipulate. Because of this, you should prefer to store your data in machine-readable formats. Avoid .xlsx files for storing data. Prefer TXT, CSV, TSV, JSON, YAML, and HDF5.\nIf you have very large text files then you can use compression utilities to save space. Most bioinformatics software is designed to work well with gzip compressed data. gzip is a relatively old compression format. You could also consider using xz as a means to compress your data - just know that xz compression is less supported across tools.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-naming",
    "href": "data-best-practices.html#file-naming",
    "title": "‘Best’ practices for data projects",
    "section": "File naming",
    "text": "File naming\nFile naming is important but often overlooked. You want your files to be named logically and communicate their contents. You also want your files to be named in a way that a computer can easily read. For example, spaces in filenames are a royal pain when manipulating files on the command line.\nTo ensure filenames are computer friendly, don’t use spaces in filenames. Use only letters, numbers, and “-” “_” and “.” as delimiters. For example:\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#more-on-filenames",
    "href": "data-best-practices.html#more-on-filenames",
    "title": "‘Best’ practices for data projects",
    "section": "More on filenames",
    "text": "More on filenames\nIt cannot be stressed enough how important filenames can be for an analysis. To get the most out of your files and to avoid catastrpohic failures, you should stick to some basic principles for naming files. First, use consistent and unique identifiers across all files that you generate for an experiment. For example, if you’re conducting a study that has both RNA-seq and ATAC-seq data performed on the same subjects, don’t name the files from the RNA-seq experiment subject1.fq.fz and the files from the ATAC-seq experiment control_subject1.fq.gz if they refer to the same sample. For small projects, it’s fairly easy to create consistent and unique IDs for each subject. For large projects unique random IDs can be used.\nFor example, the following filenames would be bad:\nsubject1-control_at_96hr1.txt\ns1_ctl-at-4days_2.txt\ns2TRT4d1.txt\nsbj2_Treatment_4_Days_Replicate_2.txt\nInstead, look at these filenames.\nsubject1_control_4days_rep1.txt\nsubject1_control_4days_rep2.txt\nsubject2_treatment_4days_rep1.txt\nsubject2_treatment_4days_rep2.txt\nThese are better. Why are they better? They are consistent. The delimiter is consistent between the words (“_“) and each of the words represents something meaningful about the sample. These filenames also do not contain any spaces and can easily be parsed automatically.\nFile naming best practices also apply to naming executable scripts. The name of the file should describe the function of the script. For example,\n01_align_with_STAR.sh\nis better than simply naming the file\n01_script.sh\nPro-tip\nOne easy way to create unique random IDs for a large project is to concatenate descriptions and take the SHA/MDA5 hashsum.\necho \"subject1_control_4days_rep1\" | sha256\n# 57f458a294542b2ed6ac14ca64d3c8e4599eed7a\n\necho \"subject1_control_4days_rep2\" | shasum\n# b6ea9d729e57cce68b37de390d56c542bc17dea6",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis friendly data - tidy data",
    "text": "Create analysis friendly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#untidy-data",
    "href": "data-best-practices.html#untidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Untidy(?) data",
    "text": "Untidy(?) data\nSome data formats are not amenable to the ‘tidy’ structure, i.e. they’re just not best represented as long tables. For example, large/sparse matrices, geo-spatial data, R objects, etc.the lesson here is to store data in the format that is most appropriate for the data. For example, don’t convert a matrix to a long format and save as a tsv file! Save it as an .rds file instead. Large matrices can also be efficiently stored as HDF5 files. Sparse matrices can be saved and accessed eficiently using the Matrix package in R. And if you are accessing the same data often, consider storing as a SQLite database and accessing with dbplyr or sqlalchemy in Python. The main point is don’t force data into a format that you’re familiar with only because you’re familiar with that format. This will often lead to large file sizes and inefficient performance.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#save-and-lock-all-raw-data",
    "href": "data-best-practices.html#save-and-lock-all-raw-data",
    "title": "‘Best’ practices for data projects",
    "section": "Save and lock all raw data",
    "text": "Save and lock all raw data\nKeep raw data in its unedited form. This includes not making changes to filenames. In bioinformatics, it’s common to get data from a sequencing facility with incomprehensible filenames. Don’t fall victim to the temptation of changing these filenames! Instead, it’s much better to keep the filenames exactly how they were sent to you and simply create a spreadsheet that maps the files to their metadata. In the case of a sample mix-up, it’s much easier to make a change to a row in a spreadsheet then to track down all of the filenames that you changed and ensure they’re correctly modified.\nOnce you have your raw data, you don’t want the raw data to change in any way that is not documented by code. To ensure this, you can consider changing file permissions to make the file immutable (unchangable). Using bash, you can change file permissions with:\nchattr +i myfile.txt\nIf you’re using Excel for data analysis, lock the spreadsheet with the raw data and only make references to this sheet when performing calculations.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#large-files",
    "href": "data-best-practices.html#large-files",
    "title": "‘Best’ practices for data projects",
    "section": "Large files",
    "text": "Large files\nYou’ll probably be dealing with files on the order of 10s of GBs. You do not want to be copying these files from one place to another. This increases confusion and runs the risk of introducing errors. Instead avoid making copies of large local files or persistent databases and simply link to the files.\nYou can use use soft links. A powerful way of finding an linking files can be done with find\n# Link all fastq files to a local directory\nfind /path/to/fq/files -name \"*.fq.gz\") -exec ln -s {} . \\;\nIf using R, you can also sometimes specify a URL in place of a file path for certain functions.\n# Avoid downloading a large GTF file - reads GTF directly into memory\nurl &lt;- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\ngtf &lt;- rtracklayer::import(url)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#backups",
    "href": "data-best-practices.html#backups",
    "title": "‘Best’ practices for data projects",
    "section": "Backups",
    "text": "Backups\n\nThere are two types of people, those who do backups and those who will do backups.\n\nThe following are NOT backup solutions:\n\nCopies of data on the same disk\nDropbox/Google Drive\nRAID arrays\n\nAll of these solutions mirror the data. Corruption or ransomware will propagate. For example, if you corrupt a file on your local computer and then push that change to DropBox then the file on DropBox is now also corrupted. I’m sure some of these cloud providers have version controlled files but it’s better to just avoid the problem entirely by keeping good backups.\n\nUse the 3-2-1 rule:\n\nKeep 3 copies of any important file: 1 primary and 2 backups.\nKeep the files on 2 different media types to protect against different types of hazards.\nStore 1 copy offsite (e.g., outside your home or business facility).\n\nA backup is only a backup if you can restore the files!",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#look-familiar",
    "href": "data-best-practices.html#look-familiar",
    "title": "‘Best’ practices for data projects",
    "section": "Look familiar?",
    "text": "Look familiar?",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure",
    "href": "data-best-practices.html#project-structure",
    "title": "‘Best’ practices for data projects",
    "section": "Project structure",
    "text": "Project structure\nOne of the most useful changes that you can make to your workflow is the create a consistent folder structure for all of your analyses and stick with it. Coming up with a consistent and generalizable structure can be challenging at first but some general guidelines are presented here and here\nFirst of all, when beginning a new project, you should have some way of naming your projects. One good way of naming projects is to each project a descriptive name and append the date the project was started. For example, brca_rnaseq_2023-11-09/ is better than rnaseq_data/. In six months when a collaborator wants their old BRCA data re-analyzed you’ll thank yourself for timestamping the project folder and giving it a descriptive name.\nMy personal structure for every project looks like:\nyyyymmdd_project-name/\n├── data\n├── doc\n├── README\n├── results\n│   ├── data-files\n│   ├── figures\n│   └── rds-files\n└── scripts\n\n\nPrefixing the project directory with the ISO date allows for easy sorting by date\na README text file is present at the top level of the directory with a short description about the project and any notes or updates\ndata/ should contain soft links to any raw data or the results of downloading data from an external source\ndoc/ contains metadata documents about the samples or other metadata information about the experiment\nresults/ contains only data generated within the project. It has sub-directories for figures/, data-files/ and rds-files/. If you have a longer or more complicated analysis then add sub-directories indicating which script generated the results.\nscripts/ contains all analysis scripts numbered in their order of execution. Synchronize the script names with the results they produce.\n\n\nA more complex example\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "href": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "title": "‘Best’ practices for data projects",
    "section": "Record all steps used to generate the data",
    "text": "Record all steps used to generate the data\nAlways document all steps you used to generate the data that’s present in your projects. This can be as simple as a README with some comments and a wget command or as complex as a snakemake workflow. The point is, be sure you can track down the exact source of every file that you created or downloaded.\nFor example, a README documenting the creation of the files needed to generate a reference index might look like:\nTranscripts:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\n\nPrimary Assembly:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\n\nCreate concatenated transcripts + genome for salmon (i.e. gentrome):\ncat gencode.v38.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz &gt; gentrome.fa.gz\n\nCreate decoys file for salmon:\ngrep \"&gt;\" &lt;(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 &gt; decoys.txt\nsed -i.bak -e 's/&gt;//g' decoys.txt\nFor more complicated steps include a script. e.g. creating a new genome index, subsetting BAM files, accessing data from NCBI, etc.\nPro-tip\nA simple way to build a data pipeline that is surprisingly robust is just to create scripts for each step and number them in the order that they should be executed.\n01_download.sh\n02_process.py\n03_makeFigures.R\nYou can also include a runner script that will execute all of the above. Or, for more consistent workflows, use a workflow manager like Nextflow, Snakemake, WDL, or good ole’ GNU Make",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#manual-version-control",
    "href": "data-best-practices.html#manual-version-control",
    "title": "‘Best’ practices for data projects",
    "section": "Manual version control",
    "text": "Manual version control\nVersion control refers to the practice of tracking changes in files and data over their lifetime. You should always track any changes made to your project over the entire life of the project. This can be done either manually or using a dedicated version control system. If doing this manually, add a file called “CHANGELOG.md” in your docs/ directory and add detailed notes in reverse chronological order.\nFor example:\n## 2016-04-08\n\n* Switched to cubic interpolation as default.\n* Moved question about family's TB history to end of questionnaire.\n\n## 2016-04-06\n\n* Added option for cubic interpolation.\n* Removed question about staph exposure (can be inferred from blood test results).\n\nIf you make a significant change to the project, copy the whole directory, date it, and store it such that it will no longer be modified. Copies of these old projects can be compressed and saved with tar + xz compression\ntar -cJvf old.20231109_myproject.tar.xz myproject/`",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#version-control-with-git",
    "href": "data-best-practices.html#version-control-with-git",
    "title": "‘Best’ practices for data projects",
    "section": "Version control with git",
    "text": "Version control with git\n\ngit is probably the de facto version control system in use today for tracking changes across software projects. You should strive to learn and use git to track your projects. Version control systems allow you to track all changes, comment on why changes were made, create parallel branches, and merge existing ones.\ngit is primarily used for source code files. Microsoft Office files and PDFs can be stored with Github but it’s hard to track changes. Rely on Microsoft’s “Track Changes” instead and save frequently.\nIt’s not necessary to version control raw data (back it up!) since it shouldn’t change. Likewise, backup intermediate data and version control the scripts that made it.\nFor a quick primer on Git and GitHub check out the book Happy Git with R or The Official GitHub Training Manual Anyone in the lab can join the coriell-research organization on Github and start tracking their code\nBe careful committing sensitive information to GitHub",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "title": "‘Best’ practices for data projects",
    "section": "Quick tips to improve your scripts",
    "text": "Quick tips to improve your scripts\n\nPlace a description at the top of every script\nThe description should indicate who the author is. When the code was created. A short description of what the expected inputs and outputs are along with how to use the code. You three months from now will appreciate it when you need to revisit your analysis\nFor example:\n#!/usr/bin/env python3\n# Gennaro Calendo\n# 2023-11-09\n# \n# This scripts performs background correction of all images in the \n#  user supplied directory\n#\n# Usage ./correct-bg.py --input images/ --out_dir out_directory\n#\nfrom image_correction import background_correct\n\nfor img in images:\n  img = background_correct(img)\n  save_image(img, \"out_directory/corrected.png\")\n  \n\n\nDecompose programs into functions\nFunctions make it easier to reason about your code, spot errors, and make changes. This also follows the Don’t Repeat Yourself principle aimed at reducing repetition by replacing it with abstractions that are more stable\nCompare this chunk of code that rescales values using a min-max function (0-1)\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\nto this function which does the same thing\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf &lt;- lapply(df, rescale01)\nWhich is easier to read? Which is easier to debug? Which is more efficient?\n\n\nGive functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol &lt;- 1:100\n\nmydata &lt;- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages &lt;- 1:100\n\nbioinfo_names &lt;- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\n\n\nDo not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename &lt;- \"data.tsv\"\n#url &lt;- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf &lt;- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename &lt;- \"data.tsv\"\nurl &lt;- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf &lt;- read.delim(filename)\n\n\nUse a consistent style\nPick a style guide and stick with it. If using R, the styler package can automatically clean up poorly formatted code. If using Python, black is a highly opinionated formatter that is pretty popular. Although, I think ruff is currently all the rage with the Pythonistas these days.\nBad:\nflights|&gt;group_by(dest)|&gt; summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |&gt; \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-&gt; flight_plot\nGood:\nflight_plot &lt;- flights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n\nDon’t use right hand assignment\nThis is R specific. I’ve seen this pop up with folks who are strong tidyverse adherents. I get it, that’s the direction of the piping operator. However, this right-hand assignment flies in the face of basically every other programming language, and since code is primarily read rather than executed, it’s much harder to scan a codebase and understand the variable assignment when the assignments can be anywhere in the pipe!\nDon’t do this\ndata |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...) -&gt; by_group\nIt’s much easier to look down a script and see that by_group is created by all of the piped operations when assigned normally.\nby_group &lt;- data |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#best-practices",
    "href": "data-best-practices.html#best-practices",
    "title": "‘Best’ practices for data projects",
    "section": "Best practices",
    "text": "Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#bioinformatics",
    "href": "data-best-practices.html#bioinformatics",
    "title": "‘Best’ practices for data projects",
    "section": "Bioinformatics",
    "text": "Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#proficiency-with-computational-tools",
    "href": "data-best-practices.html#proficiency-with-computational-tools",
    "title": "‘Best’ practices for data projects",
    "section": "Proficiency with computational tools",
    "text": "Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#r",
    "href": "data-best-practices.html#r",
    "title": "‘Best’ practices for data projects",
    "section": "R",
    "text": "R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "linux.html",
    "href": "linux.html",
    "title": "Command line basics",
    "section": "",
    "text": "Accessing the terminal\nBasic proficiency with the Unix shell is essential for anyone who wants to start doing computational work outside of their laptop and Excel. Unix shells provide an interface for interacting with Unix-like (Mac OS, Linux, etc.) operating systems and a scripting language for controlling the system. The Unix philosopy is a set of software engineering norms and concepts that guide how the tools of the Unix shell interact with one another. Learning a few of these command line tools, and how they can be strung together into what are called “pipes”, is a powerful skill for developing quick and composable bioinformatics programs. Here, we’ll describe some essential commands to get you started using the command line.\nFirst, you’ll have to open the Terminal application. If you’re on Mac OS, the quickest way to access your terminal is: “command + space”, typing “terminal” and pressing Enter. On Windows, you’ll have to install Windows Subsystem for Linux which will allow you to interact with a (default) Ubuntu OS.\nOnce you’ve opened the terminal app, you’re ready to start typing commands at the command line.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#where-am-i",
    "href": "linux.html#where-am-i",
    "title": "Command line basics",
    "section": "Where am I?",
    "text": "Where am I?\nThe first command you should know is pwd. pwd will print your current working directory. This command is used to display where you are currently in the file system. For example, if I open a terminal window in my “Downloads” directory and type and hit Enter:\npwd \nIt will return\n/home/gennaro/Downloads\nindicating that I am in my “Downloads” directory.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#listing-files",
    "href": "linux.html#listing-files",
    "title": "Command line basics",
    "section": "Listing files",
    "text": "Listing files\nNow that I’m in my “Downloads” directory I want to see what files I’ve downloaded. To do this, I can use the ls command to list files in the directory.\nls\nwhich returns:\nBDNF-data.tsv  CORI_Candidate_SNP_draft_250528_clean.docx  differential-expression2.tsv\nYour “Downloads” directory will of course have different files. If I need to display more information about these files, such as the time that they were created or how large they are, I can supply the ls command with arguments.\nFor example\nls -lah\nReturns\ntotal 15M\ndrwxr-xr-x  2 gennaro gennaro 4.0K Jun  1 14:52 .\ndrwxr-x--- 51 gennaro gennaro 4.0K Jun  1 09:34 ..\n-rw-rw-r--  1 gennaro gennaro 6.8K May 30 18:00 BDNF-data.tsv\n-rw-rw-r--  1 gennaro gennaro 973K May 30 17:34 CORI_Candidate_SNP_draft_250528_clean.docx\n-rw-rw-r--  1 gennaro gennaro  14M May 30 12:54 differential-expression2.tsv\nWhich provides information about the file permissions, the file sizes, and when the files were created.\n\nLearning more about a command\nTo learn more about what arguments are available to any of the command line programs you run, you can use the man, or manual, command. This command will open the user manual for the given command.\nTry typing\nman ls\nto view all of the options available when listing files with ls.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-around",
    "href": "linux.html#moving-around",
    "title": "Command line basics",
    "section": "Moving around",
    "text": "Moving around\nLet’s say I want to move from my “Downloads” directory to my “Documents” directory. The command I have to use is cd, short for “change directory”. We can use the cd command with the argument for the target directory we want to go to. For example, to move to my “Documents” directory\ncd /home/gennaro/Documents\n\nDirectory shortcuts\nThe shell has a few shortcuts that make moving around a little easier. Running cd without any arguments will bring you back into your home directory.\ncd\nIn Bash, there is an additional shortcut to specify the “/home” as well. You can use ~ in place of “/home”. For example, to move into my “Documents” folder I can use\ncd ~/Documents\ninstead of typing the full path. To go up one level in the directory you can use ... So to go from my Documents directory ‘up’ into my “/home” directory I can use\ncd ..\nFinally, to go back to the same directory that you were just in you can use\ncd -",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#creating-files",
    "href": "linux.html#creating-files",
    "title": "Command line basics",
    "section": "Creating files",
    "text": "Creating files\nYou can create files with the touch command. For example, to create an empty file in my “Downloads” directory called “A.txt” I can run\ntouch ~/Downloads/A.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#redirection",
    "href": "linux.html#redirection",
    "title": "Command line basics",
    "section": "Redirection",
    "text": "Redirection\nI’ll add some content to this file using the echo command. echo simply prints it’s arguments back out to the terminal. I’ll also use what is called redirection to append the results of the echo command into the text file.\nRedirection is a core concept in Unix pipes. It allows you to take the output from one program and use it as input to another program. In this example, I’ll take the output from echo and redirect it to the file “A.txt” that we just created.\necho \"This is a new line in the file\" &gt;&gt; ~/Downloads/A.txt\necho \"Here is another new line in the file\" &gt;&gt; ~/Downloads/A.txt\nThe &gt;&gt; took the output of the echo command and inserted it as a new line in “A.txt”. Importantly, &gt;&gt; appended these lines into “A.txt”. If I were instead to use &gt; like\necho \"This will replace the current contents of A.txt\" &gt; ~/Downloads/A.txt\n“A.txt” will be overwritten with the new contents. The final essential redirection operator is the pipe |. The pipe lets you take the output from one program and use it as input to another. I’ll show an example of this later.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#displaying-the-content-of-files",
    "href": "linux.html#displaying-the-content-of-files",
    "title": "Command line basics",
    "section": "Displaying the content of files",
    "text": "Displaying the content of files\nThe simplest way to display the contents of a file on the command line is by using the cat command. The cat command is actually designed to concatenate file together, but running it on a single file will print the entire contents of the file to the command line. For example, to print the contents of “A.txt”\ncat ~/Downloads/A.txt\nWill print\nThis will replace the current contents of A.txt\nto the console. If you have a lot of text that you would like to display cat can result in too much information being displayed on the screen. Instead, you can use the less command. less will print the contents of the file as pages on the screen. You can use the d key to scroll down a page, or the u key to scroll up a page.\nAnother way to display only some of the contents of a file is to use the head or tail commands. head -n10 will print the first 10 lines of a file, whereas tail -n10 can be used to print the last 10 lines of a file.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#copying-files",
    "href": "linux.html#copying-files",
    "title": "Command line basics",
    "section": "Copying files",
    "text": "Copying files\nYou can copy a file using the cp command. For example, to copy the “A.txt” file into a new file “B.txt” I can use\ncp ~/Downloads/A.txt ~/Downloads/B.txt\nTo copy an entire directory you need to supply the -r, or recursive, argument to the cp command. For example, to create a copy of my Downloads directory inside of my Documents directory\ncp -r ~/Downloads ~/Documents/Downloads-copy",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-and-renaming-files",
    "href": "linux.html#moving-and-renaming-files",
    "title": "Command line basics",
    "section": "Moving and renaming files",
    "text": "Moving and renaming files\nThe mv command can be used to move files and rename them. For example, to move the “A.txt” file into my Documents directory I can use\nmv A.txt ~/Documents\nIf I now want to change the name of that file I can also use the mv command. Now you need to specify the new file name instead of the location to move the file to\nmv ~/Documents/A.txt ~/Documents/C.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#making-new-directories",
    "href": "linux.html#making-new-directories",
    "title": "Command line basics",
    "section": "Making new directories",
    "text": "Making new directories\nTo make a new directory you can use the mkdir command. To make a new directory inside of my Downloads directory I can use\nmkdir ~/Downloads/textfiles\nBy default, the mkdir command doesn’t allow you to create nested directories. To enable this, set the mkdir -p flag. For example I can create a parent folder and subfolders using\nmkdir -p ~/Downloads/imagefiles/jpegs\n\nShell expansion\nAnother useful trick is to learn shell expansion. Shell expansion ‘expands’ the arguments. Shell expansion can be a shortcut when creating new project directories. For example\nmkdir -p data doc scripts results/{figures,data-files,rds-files}\nThe results/{figures,data-files,rds-files} expands this command into\nmkdir -p data doc scripts results/figures results/data-files results/rds-files\nWhich saves some typing. Shell expansion can also be used in other contexts. For example, I can create 260 empty text files using the following command\ntouch ~/Downloads/textfiles/{A..Z}{1..10}.txt\nAnother useful shell expansion is *. For example, if I needed to display the contents of each of the file we just created I could run\ncat ~/Downloads/textfiles/*.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#removing-files",
    "href": "linux.html#removing-files",
    "title": "Command line basics",
    "section": "Removing files",
    "text": "Removing files\nRemoving files on the command line can be done with the rm command. Unlike when using a GUI, when you remove files on the command line you cannot get them back so use rm wisely. To remove one of the empty files I just created I can use\nrm ~/Downloads/textfiles/A1.txt\nIf I want to remove the entire “textfiles” directory I can use the -r, or recursive flag with rm.\nrm -r ~/Downloads/textfiles\nBe careful when using rm. A simple space can mean removing entire file systems by mistake!",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#finding-files",
    "href": "linux.html#finding-files",
    "title": "Command line basics",
    "section": "Finding files",
    "text": "Finding files\nOne incredibly useful but often overlooked command line tools is find. find does exactly what you expect it to do, it finds files and folders. find has many arguments but the simplest usage is for finding files using a specific pattern. For example, to find all text (.txt) files in a particular directory and all of its subdirectories you can use:\nfind . -name \"*.txt\" -type f\nThis command says, “find any file (-type f) that has a name like ‘.txt’”. find is especially powerful when combined with the -exec argument. For example, to remove all .txt file in a directory you can use:\nfind . -name \"*.txt\" -type f -exec rm {} \\;",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#downloading-files",
    "href": "linux.html#downloading-files",
    "title": "Command line basics",
    "section": "Downloading files",
    "text": "Downloading files\ncurl and wget are both command line utilities for downloading files from remote resources. curl will download and stream the results to your terminal by default. wget will save the result to a file by default.\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt\nwget https://www.gutenberg.org/cache/epub/100/pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#searching-the-contents-of-files",
    "href": "linux.html#searching-the-contents-of-files",
    "title": "Command line basics",
    "section": "Searching the contents of files",
    "text": "Searching the contents of files\ngrep is a tool that’s use to search the contents of files for specific text patterns. For example, if you wanted to find every line in a text file that contains the word “the” you could use:\ngrep \"the\" pg100.txt\ngrep also has many useful arguments. One of the most useful is that grep can return the count of the number of lines that are returned. For example, to count the number of lines in a text file that contain the word “the”:\ngrep -c \"the\" pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#replacing-file-contents",
    "href": "linux.html#replacing-file-contents",
    "title": "Command line basics",
    "section": "Replacing file contents",
    "text": "Replacing file contents\nsed 's/find/replace/' myfile.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#piping-commands",
    "href": "linux.html#piping-commands",
    "title": "Command line basics",
    "section": "Piping commands",
    "text": "Piping commands\nThe Unix pipe is what makes the command line so powerful. You can string together small programs to build up solutions to complex problems. The pipe allows you to take the output from one program and use it as input to another program directly.\nFor example, suppose we wanted count the top 10 most frequently used words across all of the works of Shakespeare\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt | \\\nsed 's/[^a-zA-Z ]/ /g' | \\\ntr 'A-Z ' 'a-z\\n' | \\\ngrep '[a-z]' | \\\nsort | \\\nuniq -c | \\\nsort -nr -k1 | \\\nhead -n10\n\ncurl downloads the text file from Project Gutenberg and streams it to stdout\nsed replaces all characters that are not spaces or letters, with spaces.\ntr changes all of the uppercase letters into lowercase and converts the spaces in the lines of text to newlines (each ‘word’ is now on a separate line)\ngrep includes only lines that contain at least one lowercase alphabetical character (removing any blank lines)\nsort sorts the list of ‘words’ into alphabetical order\nuniq counts the occurrences of each word\nsort sorts the occurrences numerically in descending order\nhead shows the top 10 lines",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#compressing-and-uncompressing",
    "href": "linux.html#compressing-and-uncompressing",
    "title": "Command line basics",
    "section": "Compressing and uncompressing",
    "text": "Compressing and uncompressing\nBioinformatics and command line tools can generally work with compressed data. Data compression saves space which can be really beneficial when transferring files over the internet. gzip is an old but commonly used compression utility that is compatible with many command line utilities. To compress a file for example.\ngzip pg100.txt\nWill produce a compressed version of the “pg100.txt” file called “pg100.txt.gz”. Many Unix tools can work directly with gzipped files. For example,\nzcat pg100.txt.gz\nWill unzip and print the contents of the file to the terminal and zgrep can be used to directly search the contents of a gzipped file without the need to decompress the entire file\nzgrep -c \"the\" pg100.txt.gz\nTo decompress a file you use the ‘un’ version of the compression command, gunzip.\ngunzip somefile.txt.gz",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#for-loops",
    "href": "linux.html#for-loops",
    "title": "Command line basics",
    "section": "For-loops",
    "text": "For-loops\nThe command line is also a scripting language and like any scripting language, it provides some basic control flow utilities. One of the more useful of these is the basic for loop. In Bash, the for-loop takes the form of a for each loop. the looping variable can be referred to in the loop by using the $ syntax. For example, to loop through\nfor F in *.txt; do sort $F | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt; done",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#gnu-parallel",
    "href": "linux.html#gnu-parallel",
    "title": "Command line basics",
    "section": "GNU parallel",
    "text": "GNU parallel\nGNU parallel is a command line tool that takes away the need to use for-loops entirely. parallel is extremely powerful and feature filled. Importantly, it lets you run commands across multiple jobs. For example, instead of writing a for loop we can process the text files above using 8 jobs at once with parallel\nparallel --jobs 8 \"sort {} | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt\" ::: *.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#editors",
    "href": "linux.html#editors",
    "title": "Command line basics",
    "section": "Editors",
    "text": "Editors\nYou’ll eventually need to edit some code or files from the terminal. Two options for code editing from the command line are vim and nano. vim can be more difficult to use for a beginner but is very powerful.\nvim\nOnce you’re in vim you can use the i key to enter “input” mode. “input” mode let’s you type in new characters. Once you’ve typed away, save your work and exit with esc + :wq. vim can be difficult to get used to which lead to the most famous of StackOverflow questions: nano provides a more user friendly interface.\nnano",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#resources",
    "href": "linux.html#resources",
    "title": "Command line basics",
    "section": "Resources",
    "text": "Resources\n\nTerminus is a fun game designed to get you comfortable navigating the command line\nOverTheWire is another game designed to tech you command line tools through the lense of a ‘hacker’\nvimtutor can be used to learn vim",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "R programming basics",
    "section": "",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#subsetting-in-r",
    "href": "r-basics.html#subsetting-in-r",
    "title": "R programming basics",
    "section": "Subsetting in R",
    "text": "Subsetting in R\nYou may have only ever encountered R from the perspective of the tidyverse. tidyverse functions provide useful abstractions for munging tidy data however, most genomics data is often best represented and operated on as matrices. Keeping your data in matrix format can provide many benefits as far as speed and code clarity, which in turn helps to ensure correctness. You can think of matrices as just fancy 2D versions of vectors. So what are vectors?\nVectors are the main building blocks of most R analyses. Whenever you use the c() function (‘concatenate’), like: x &lt;- c('a', 'b', 'c') you’re creating a vector. Vectors hold R objects and are the building block of more complex structures in R.\nNOTE: the following is heavily inspired by Norm Matloff’s excellent fasteR tutorial. Take a look there to get a brief and concise overview base R. You should also check out the first few chapters of Hadley Wickham’s amazing book Advanced R. The first edition contains some more information on base R.\n\nSubsetting vectors\nBelow, we’ll use the built-in R constant called LETTERS. The LETTERS vector is simply a ‘list’ of all uppercase letters in the Roman alphabet.\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nWe can subset the vector by position. For example, to get the 3rd letter we use the [ operator and the position we want to extract.\n\nLETTERS[3]\n\n[1] \"C\"\n\n\nWe can also use a range of positions. The notation 3:7 is a shortcut that generates the numbers, 3, 4, 5, 6, 7.\n\nLETTERS[3:7]\n\n[1] \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\nWe don’t have to select sequential elements either. We can extract elements by using another vector of positions.\n\nLETTERS[c(7, 5, 14, 14, 1, 18, 15)]\n\n[1] \"G\" \"E\" \"N\" \"N\" \"A\" \"R\" \"O\"\n\n\nVectors become really powerful when we start combining them with logical operations. R supports all of the usual logical and comparison operators you can expect from a programming language, &lt;, &gt;, ==, !=, &lt;=, &gt;=, %in%, & and |.\n\nmy_favorite_letters &lt;- c(\"A\", \"B\", \"C\")\n\n# See that this produces a logical vector of (TRUE/FALSE) values\n# TRUE when LETTERS is one of my_favorite_letters and FALSE otherwise\nLETTERS %in% my_favorite_letters\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\n# We can use that same expression to filter the vector\nLETTERS[LETTERS %in% my_favorite_letters]\n\n[1] \"A\" \"B\" \"C\"\n\n\nThis same kind of subsetting works on vectors that contain numeric data as well. For example, we can filter the measurements of annual flow of water through the Nile river like so:\nNile is another built-in dataset\n\n# Any values strictly greater than 1200\nNile[Nile &gt; 1200]\n\n[1] 1210 1230 1370 1210 1250 1260 1220\n\n# Any even number - `%%` is the modulus operator\nNile[Nile %% 2 == 0]\n\n [1] 1120 1160 1210 1160 1160 1230 1370 1140 1110  994 1020  960 1180  958 1140\n[16] 1100 1210 1150 1250 1260 1220 1030 1100  774  840  874  694  940  916  692\n[31] 1020 1050  726  456  824  702 1120 1100  832  764  768  864  862  698  744\n[46]  796 1040  944  984  822 1010  676  846  812  742 1040  860  874  848  890\n[61]  744  838 1050  918  986 1020  906 1170  912  746  718  714  740\n\n\nAt this point it’s important to take a step back and appreciate what R is doing. Each of the comparison operators that we used above is vectorized. This means that the comparison is applied to all elements of the vector at one time. If you’re used to a programming language like Python this might seem foreign at first. In Python, you would have to write a list comprehension to filter observations from a list that meet a certain condition. For example, [x for x in Nile if x &gt; 1200]. However in R, most functions and operators are vectorized allowing us to do things like Nile &gt; 1200 and have the comparison applied to all of the elements of the vector automatically.\n\n\nSubsetting data.frames\nBut these are just one dimensional vectors. In R, we usually deal with data.frames (tibbles for you tidyverse folks) and matrices. Lucky for us, the subsetting operations we learned for vectors work the same way for data.frames and matrices.\nLet’s take a look at the built-in ToothGrowth dataset. The data consists of the length of odontoblasts in 60 guinea pigs receiving one of three levels of vitamin C by one of two delivery methods.\n\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nThe dollar sign $ is used to extract an individual column from the data.frame, which is just a vector.\n\nhead(ToothGrowth$len)\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWe can also use the [[ to get the same thing. Double-brackets come in handy when your columns are not valid R names since $ only works when columns are valid names.\n\nhead(ToothGrowth[[\"len\"]])\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWhen subsetting a data.frame in base R, the general scheme is:\ndf[the rows you want, the columns you want]\nSo in order to get the 5th row of the first column we could do:\n\nToothGrowth[5, 1]\n\n[1] 6.4\n\n\nAgain, we can combine this kind of thinking to extract rows and columns matching logical conditions. For example, if we want to get all of the animals administered orange juice (‘OJ’)\n\nToothGrowth[ToothGrowth$supp == \"OJ\", ]\n\n    len supp dose\n31 15.2   OJ  0.5\n32 21.5   OJ  0.5\n33 17.6   OJ  0.5\n34  9.7   OJ  0.5\n35 14.5   OJ  0.5\n36 10.0   OJ  0.5\n37  8.2   OJ  0.5\n38  9.4   OJ  0.5\n39 16.5   OJ  0.5\n40  9.7   OJ  0.5\n41 19.7   OJ  1.0\n42 23.3   OJ  1.0\n43 23.6   OJ  1.0\n44 26.4   OJ  1.0\n45 20.0   OJ  1.0\n46 25.2   OJ  1.0\n47 25.8   OJ  1.0\n48 21.2   OJ  1.0\n49 14.5   OJ  1.0\n50 27.3   OJ  1.0\n51 25.5   OJ  2.0\n52 26.4   OJ  2.0\n53 22.4   OJ  2.0\n54 24.5   OJ  2.0\n55 24.8   OJ  2.0\n56 30.9   OJ  2.0\n57 26.4   OJ  2.0\n58 27.3   OJ  2.0\n59 29.4   OJ  2.0\n60 23.0   OJ  2.0\n\n\nWe can also combine logical statements. For example, to get all of the rows for animals administered orange juice and with odontoblast length (‘len’) less than 10.\n\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, ]\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n# We can also use the bracket notation to select rows and columns at the same time\n# Although this gets a little difficult to read\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, c(\"len\", \"supp\")]\n\n   len supp\n34 9.7   OJ\n37 8.2   OJ\n38 9.4   OJ\n40 9.7   OJ\n\n\nIt gets annoying typing ToothGrowth every time we want to subset the data.frame. Base R has a very useful function called subset() that can help us type less. subset() essentially ‘looks inside’ the data.frame for the given columns and evaluates the expression without having to explicitly tell R where to find the columns. Think of it like dplyr::filter(), if you are familiar with that function.\n\nsubset(ToothGrowth, supp == \"OJ\" & len &lt; 10)\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n\n\n\nSubsetting Lists\nAnother data structure to be aware of, which is used frequently, is the List. We’ve actually already encountered Lists above. data.frames are really just Lists where each vector contains the same data type and all List elements are the same length.\nWe can create a List in R using the list() function. Notice how each list element has a name and can contain a different type of data and number of data elements\n\nl &lt;- list(\n  element1 = c(1, 10, 12, 3, 6, 12, 13, 2, 5, 6, 3, 7),\n  element2 = c(\"a\", \"b\", \"c\"),\n  element3 = c(TRUE, TRUE, FALSE, FALSE, FALSE),\n  element4 = c(0.001, 0.05, 0.86, 1.098, 345.0)\n)\n\nLists can be tricky at first. To extract the data from a particular list element you can use the [[ or the $ (as in the case of data.frames above). Like vectors, you can use either the index or the name of the element you wish to extract.\n\nl[[1]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl[[\"element1\"]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl$element1\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nWhat is returned if you only use the single bracket [?\n\nl[1]\n\n$element1\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nYou get another List, but now with a single element. This behavior might seem unintuitive at first, but it can be very useful for creating new lists.\n\nnumeric_l &lt;- l[c(1, 4)]\n\n\n\nSubsetting matrices\nMatrices behave much like data.frames but unlike data.frames matrices can only contain one type of data. This might sound like a limitation at first but you’ll soon come to realize that matrices are very powerful (and fast) to work with in R.\n\nset.seed(123)\n\n# Create some random data that looks like methylation values\n(m &lt;- matrix(\n  data = runif(6 * 10),\n  ncol = 6,\n  dimnames = list(\n    paste0(\"CpG.\", 1:10),\n    paste0(\"Sample\", 1:6)\n  )\n))\n\n         Sample1    Sample2   Sample3    Sample4   Sample5    Sample6\nCpG.1  0.2875775 0.95683335 0.8895393 0.96302423 0.1428000 0.04583117\nCpG.2  0.7883051 0.45333416 0.6928034 0.90229905 0.4145463 0.44220007\nCpG.3  0.4089769 0.67757064 0.6405068 0.69070528 0.4137243 0.79892485\nCpG.4  0.8830174 0.57263340 0.9942698 0.79546742 0.3688455 0.12189926\nCpG.5  0.9404673 0.10292468 0.6557058 0.02461368 0.1524447 0.56094798\nCpG.6  0.0455565 0.89982497 0.7085305 0.47779597 0.1388061 0.20653139\nCpG.7  0.5281055 0.24608773 0.5440660 0.75845954 0.2330341 0.12753165\nCpG.8  0.8924190 0.04205953 0.5941420 0.21640794 0.4659625 0.75330786\nCpG.9  0.5514350 0.32792072 0.2891597 0.31818101 0.2659726 0.89504536\nCpG.10 0.4566147 0.95450365 0.1471136 0.23162579 0.8578277 0.37446278\n\n\nIf we want to extract the value for CpG.3 for Sample3\n\nm[3, 3]\n\n[1] 0.6405068\n\n\nOr all values of CpG.3 for every sample\n\nm[3, ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n# Or refer to the row by it's name\nm[\"CpG.3\", ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n\nOr all CpGs for Sample3\n\nm[, 3]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n# Or refer to the column by it's name\nm[, \"Sample3\"]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n\nWe can also apply a mask to the entire matrix at once. For example, the following will mark any value that is greater than 0.5 with TRUE\n\nm &gt; 0.5\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nCpG.1    FALSE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.2     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.3    FALSE    TRUE    TRUE    TRUE   FALSE    TRUE\nCpG.4     TRUE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.5     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.6    FALSE    TRUE    TRUE   FALSE   FALSE   FALSE\nCpG.7     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.8     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.9     TRUE   FALSE   FALSE   FALSE   FALSE    TRUE\nCpG.10   FALSE    TRUE   FALSE   FALSE    TRUE   FALSE\n\n\nWe can use this kind of masking to filter rows of the matrix using some very helpful base R functions that operate on matrices. For example, to get only those CpGs where 3 or more samples have a value &gt; 0.5 we can use the rowSums() like so:\n\nm[rowSums(m &gt; 0.5) &gt; 3, ]\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThis pattern is very common when dealing with sequencing data. Base R functions like rowSums() and colMeans() are specialized to operate over matrices and are the most efficient way to summarize matrix data. The R package matrixStats also contains highly optimized functions for operating on matrices.\nCompare the above to the tidy solution given the same matrix.\n\ntidyr::as_tibble(m, rownames = \"CpG\") |&gt;\n  tidyr::pivot_longer(!CpG, names_to = \"SampleName\", values_to = \"beta\") |&gt;\n  dplyr::group_by(CpG) |&gt;\n  dplyr::mutate(n = sum(beta &gt; 0.5)) |&gt;\n  dplyr::filter(n &gt; 3) |&gt;\n  tidyr::pivot_wider(id_cols = CpG, names_from = \"SampleName\", values_from = \"beta\") |&gt;\n  tibble::column_to_rownames(var = \"CpG\") |&gt;\n  data.matrix()\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThere’s probably some kind of tidy solution using across() that I’m missing but this is how most of the tidy code in the wild that I have seen looks",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#data-wrangling",
    "href": "r-basics.html#data-wrangling",
    "title": "R programming basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning and exploratory data analysis. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn will apply to data.frames but also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw if you have that R version installed. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\nThe read.csv() function has many options for reading in data. If you want to learn about all of the options any particular R function has, you can prefix the function name with a ? like, ?read.csv() to bring up the help documentation.\n\n\nViewing data\nThe code above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.\nOne of the most basic ways to get an idea of the data is to summarize each variable. There are a few functions we can use to get summaries of the data. The table() function will count the number of occurrences of each type in a vector.\nFor example, how many observations of each species of penguin are in the dataset?\n\ntable(penguins$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                      152 \nChinstrap penguin (Pygoscelis antarctica) \n                                       68 \n        Gentoo penguin (Pygoscelis papua) \n                                      124 \n\n\nR also provides the typical summary functions that you would expect from a statistical programming language such as mean(), median(), min(), and max(), and length().\nFor example, what is the mean flipper length?\n\nmean(penguins$Flipper.Length..mm.)\n\n[1] NA\n\n\nOn no! This returned NA but there is clearly data in this column. What happened? Missing data is commonly observed across all data domains. NAs simply represent unknown values in this context and it’s impossible to know how to take the mean of a value that’s known with a value that’s unknown. It is for this reason that many R functions have an argument called na.rm=. Setting na.rm=TRUE in these functions tells R to ignore the NA values.\n\nmean(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n[1] 200.9152\n\n\nNow we can see that the mean flipper length is ~200 mm across all observations in the dataset. Another useful function is summary(). Running summary() on a numeric vector returns a lot of useful information.\n\nsummary(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nFinally, if you are using and IDE like Rstudio or Positron you can run the View() function on your data.frame. This will bring up an interactive data viewer.\n\n\nSplitting data\nYou may have noticed above that we computed the mean flipper length across all species of penguins. But do all species have the same mean? A common pattern in R is called “split-apply-combine”. This pattern means, split the data into groups you’re interested in, apply a function to each of those groups, and combine the results. One such function that performs this operation is called tapply(). tapply() will split the data by a given variable into groups and apply a function to each group.\nFor example, to find the mean flipper length for each species we could use\n\ntapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                 189.9536 \nChinstrap penguin (Pygoscelis antarctica) \n                                 195.8235 \n        Gentoo penguin (Pygoscelis papua) \n                                 217.1870 \n\n\nThe basic format of the tapply() function is\n\ntapply(\"data to split\", \"what to split by\", \"what to compute\")\n\nSplitting can also by applied directly to data.frames using the split() function. Let’s say we wanted to split the penguins data.frame into one data.frame for each species. We can use the split() function for this purpose.\n\nby_species &lt;- split(penguins, f = penguins$Species)\n\nThis function returns a list of data.frames, one for each species in the original data.frame. Use names(by_species) to see what each list element is named. To extract the first data.frame from this list, which contains Adelie penguin data only, we can subset the list of data.frames.\n\nadelie &lt;- by_species[[1]]\n\nPerforming operations on lists is such a common task in R that a function exists specifically to apply functions to list elements. This function is called lapply(). lapply() takes a list and a function and applies that function to each list element. The lapply() function returns the results as a list. We’ll learn more about this later in the functional programming section.\nFor example, to see how many rows are in each of the data.frames in the “by_species” list:\n\nlapply(by_species, nrow)\n\n$`Adelie Penguin (Pygoscelis adeliae)`\n[1] 152\n\n$`Chinstrap penguin (Pygoscelis antarctica)`\n[1] 68\n\n$`Gentoo penguin (Pygoscelis papua)`\n[1] 124\n\n\n\n\nPlotting data\nData cleaning and data visualization go hand-in-hand. To effectively clean data, you should be examining the changes you’re making in real time. Base R actually has very powerful graphics capabilities for quickly visualizing data. Packages like ggplot2 and lattice provide powerful alternatives to base R plots. We’ll cover ggplot2 later. For now, base R plotting can provide all we need for exploratory analyses.\nOne of the most useful plots for numeric data is a histogram. Histograms bin the data and plot how many occurrences of a particular bin are present. This plot allows you to get an idea of the numeric summary of a variable. To plot a histogram of a numeric variable we can use the hist() function.\n\nhist(penguins$Flipper.Length..mm.)\n\n\n\n\n\n\n\n\nThe histogram has a few arguments we can use to adjust the plot. Use ?hist() to see the full list. Below, we can adjust the axes to be more informative and modify the number of bins we’re computing.\n\nhist(penguins$Flipper.Length..mm., \n     breaks = 30,\n     main = \"Flipper Length of All Palmer Penguins\", \n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Count\"\n     )\n\n\n\n\n\n\n\n\nThe distribution appears to be bimodal. Is this because there are different species present in this plot? We can check by applying the hist() function to each of the groups using the list of data.frames from above.\n\n# Adelie penguins \nhist(by_species[[1]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Adelie\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Chinstrap penguins\nhist(by_species[[2]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Chinstrap\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Gentoo penguins\nhist(by_species[[3]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Gentoo\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nHistograms are useful for plotting the distribution of a single numeric variable. Often, we wish to see how two variables are related. The plot() function provides a way to create a scatter plot of two variables against each other on the same plot. For example, to plot the culmen length vs the culmen depth:\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nAgain, these data points seem to be split by the different species of penguins present in the dataset. We can color these points using an additional variable in the call to the plot() function.\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     col = factor(penguins$Species),\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nYou may have noticed that we did something special to that vector of species. We wrapped it in the factor() function. In R, factors are used when we have categorical values. R uses factors to represent the different levels of each category. It may not seem important now, but factors are very useful for statistical analysis. We’ll build on this topic shortly.\nThe plot() function is very versatile. You can use it to create line plots as well, to show trends of variables over time. Here is a contrived example of using plot() to create a simple line chart.\n\nplot(\n  x = adelie$Sample.Number, \n  y = adelie$Body.Mass..g., \n  type = \"l\", \n  main = \"Body mass vs sample number in Adelie penguins\",\n  xlab = \"Sample Number\",\n  ylab = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\nScatter plots are useful for displaying two numeric values against each other. However you’ll often want to compare numeric values across categorical values. One such plot for doing this is called a boxplot. Boxplots show the median and interquartile (IQR) range for a set of data. The ‘whiskers’ of the boxplot display the 1.5 x IQR of the data. We can plot a boxplot of the flipper length for each species using the boxplot() function.\n\nboxplot(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  main = \"Flipper length by species\"\n  )\n\n\n\n\n\n\n\n\nThe boxplot() function is a little funny because it uses formula notation. Many functions in R accept formula notation as input. In this case, the formula notation means “plot the flipper length as a function of the species type”. You may have also noticed that we gave this function the penguins data.frame as the value for the data= argument. This allowed the boxplot function to use the column names without us having to explicitly say that they came from the penguins data.frame.\nIf there’s not too much data, boxplots can actually hide a lot of information. In this case, another option is to use a stripchart. A stripchart shows every data point on the plot instead of a depiction of the distribution as in the boxplot.\n\nstripchart(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  method = \"jitter\",\n  pch = 1,\n  main = \"Flipper length by species\",\n  vertical = TRUE\n  )\n\n\n\n\n\n\n\n\nBar charts are another common way that people show summaries of data. To display a barplot of data, we can use the barplot() function. To create a barplot of the mean flipper length for each species we first need to summarize the flipper length for each group and then supply these summaries to the barplot() function.\n\nspecies_means &lt;- tapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\nbarplot(species_means, main = \"Mean flipper length by species\")\n\n\n\n\n\n\n\n\nThere are many ways to modify these default plot types and make the charts look great. Take the time to look into some of base R graphing capabilities. A really good tutorial for learning about base R plotting can be found here. Also take a look at ?pairs(), ?dotchart(), and ?coplot() for some other useful base R graphics functions for creating quick plots.\n\n\nFiltering data\nWe’ve already learned how to subset a data.frame but let’s just take a step back and quickly revisit subsetting data.frames. To subset a data.frame means to select rows of a data.frame based on some condition that you’re interested in. This subsetting can be accomplished using the [ operator and setting conditions by specifying df[the rows we want, the cols we want] syntax.\nFor example, in the above plots it looked like the Gentoo penguins typically had the largest flippers. It looked like a value &gt; 205 roughly separated the Gentoo penguins from the others. How could we select the rows where the flipper length is greater than or equal to 205 and count the species types?\n\nbig_flippers &lt;- penguins[penguins$Flipper.Length..mm. &gt;= 205, ]\ntable(big_flippers$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                        3 \nChinstrap penguin (Pygoscelis antarctica) \n                                        8 \n        Gentoo penguin (Pygoscelis papua) \n                                      122 \n\n\nWe can combine conditions as well to select on multiple conditions. For example, let’s select the female penguins with flippers &gt;= 205 mm.\n\nhead(penguins[penguins$Sex == \"FEMALE\" & penguins$Flipper.Length..mm. &gt;= 205, ])\n\n    studyName Sample.Number                           Species Region Island\nNA       &lt;NA&gt;            NA                              &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n153   PAL0708             1 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n155   PAL0708             3 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n158   PAL0708             6 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n159   PAL0708             7 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n161   PAL0708             9 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\nNA                &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;       &lt;NA&gt;\n153 Adult, 1 Egg Stage         N31A1               Yes 2007-11-27\n155 Adult, 1 Egg Stage         N32A1               Yes 2007-11-27\n158 Adult, 1 Egg Stage         N33A2               Yes 2007-11-18\n159 Adult, 1 Egg Stage         N34A1               Yes 2007-11-27\n161 Adult, 1 Egg Stage         N35A1               Yes 2007-11-27\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.\nNA                  NA                NA                  NA            NA\n153               46.1              13.2                 211          4500\n155               48.7              14.1                 210          4450\n158               46.5              13.5                 210          4550\n159               45.4              14.6                 211          4800\n161               43.3              13.4                 209          4400\n       Sex Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\nNA    &lt;NA&gt;                NA                NA     &lt;NA&gt;\n153 FEMALE           7.99300         -25.51390     &lt;NA&gt;\n155 FEMALE           8.14705         -25.46172     &lt;NA&gt;\n158 FEMALE           7.99530         -25.32829     &lt;NA&gt;\n159 FEMALE           8.24515         -25.46782     &lt;NA&gt;\n161 FEMALE           8.13643         -25.32176     &lt;NA&gt;\n\n\nAnother important aspect of base R data.frames is that they can utilize rownames(). Using rownames() is uncommon in workflows from the tidyverse or even data.table. However, rownames() can be very useful when dealing with bioinformatics data. One R package that utilizes rownames() extensively is the SummarizedExperiment. SummarizedExperiments use rownames and colnames to ensure that data stays coordinated during an analysis.\nYou can view and set the rownames() on a data.frame using the rownames() function.\n\n# View the current rownames\nhead(rownames(penguins))\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n# Set the rownames based on a new value - we'll go over what paste() does shortly\nrownames(penguins) &lt;- paste(\"row_number\", 1:nrow(penguins), sep = \".\")\n\n# View the new rownames\nhead(rownames(penguins))\n\n[1] \"row_number.1\" \"row_number.2\" \"row_number.3\" \"row_number.4\" \"row_number.5\"\n[6] \"row_number.6\"\n\n\nAbove, we filtered by a column in the data.frame. We can also filter by selecting rownames.\n\npenguins[c(\"row_number.1\", \"row_number.5\"), ]\n\n             studyName Sample.Number                             Species Region\nrow_number.1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers\nrow_number.5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers\n                Island              Stage Individual.ID Clutch.Completion\nrow_number.1 Torgersen Adult, 1 Egg Stage          N1A1               Yes\nrow_number.5 Torgersen Adult, 1 Egg Stage          N3A1               Yes\n               Date.Egg Culmen.Length..mm. Culmen.Depth..mm.\nrow_number.1 2007-11-11               39.1              18.7\nrow_number.5 2007-11-16               36.7              19.3\n             Flipper.Length..mm. Body.Mass..g.    Sex Delta.15.N..o.oo.\nrow_number.1                 181          3750   MALE                NA\nrow_number.5                 193          3450 FEMALE           8.76651\n             Delta.13.C..o.oo.                       Comments\nrow_number.1                NA Not enough blood for isotopes.\nrow_number.5         -25.32426                           &lt;NA&gt;\n\n\n\n\nReordering data\nWe can also change the order of the rows in a data.frame. This is similar to subsetting but instead rearranges the data based on some condition. To reorder the data.frame we can use the order() function.\nFor example, we can order the data by decreasing flipper length\n\nhead(penguins[order(penguins$Flipper.Length..mm., decreasing = TRUE), ])\n\n               studyName Sample.Number                           Species Region\nrow_number.216   PAL0809            64 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.154   PAL0708             2 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.186   PAL0708            34 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.218   PAL0809            66 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.228   PAL0809            76 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.242   PAL0910            90 Gentoo penguin (Pygoscelis papua) Anvers\n               Island              Stage Individual.ID Clutch.Completion\nrow_number.216 Biscoe Adult, 1 Egg Stage         N19A2               Yes\nrow_number.154 Biscoe Adult, 1 Egg Stage         N31A2               Yes\nrow_number.186 Biscoe Adult, 1 Egg Stage         N56A2               Yes\nrow_number.218 Biscoe Adult, 1 Egg Stage         N20A2               Yes\nrow_number.228 Biscoe Adult, 1 Egg Stage         N56A2               Yes\nrow_number.242 Biscoe Adult, 1 Egg Stage         N14A2               Yes\n                 Date.Egg Culmen.Length..mm. Culmen.Depth..mm.\nrow_number.216 2008-11-13               54.3              15.7\nrow_number.154 2007-11-27               50.0              16.3\nrow_number.186 2007-12-03               59.6              17.0\nrow_number.218 2008-11-04               49.8              16.8\nrow_number.228 2008-11-06               48.6              16.0\nrow_number.242 2009-11-25               52.1              17.0\n               Flipper.Length..mm. Body.Mass..g.  Sex Delta.15.N..o.oo.\nrow_number.216                 231          5650 MALE           8.49662\nrow_number.154                 230          5700 MALE           8.14756\nrow_number.186                 230          6050 MALE           7.76843\nrow_number.218                 230          5700 MALE           8.47067\nrow_number.228                 230          5800 MALE           8.59640\nrow_number.242                 230          5550 MALE           8.27595\n               Delta.13.C..o.oo. Comments\nrow_number.216         -26.84166     &lt;NA&gt;\nrow_number.154         -25.39369     &lt;NA&gt;\nrow_number.186         -25.68210     &lt;NA&gt;\nrow_number.218         -26.69166     &lt;NA&gt;\nrow_number.228         -26.71199     &lt;NA&gt;\nrow_number.242         -26.11657     &lt;NA&gt;\n\n\nR also provides the sort() function. See if you can understand the difference between order() and sort().\nSimilar to filtering above, we can also reorder by the rownames().\n\nhead(penguins[order(rownames(penguins)), ])\n\n               studyName Sample.Number                             Species\nrow_number.1     PAL0708             1 Adelie Penguin (Pygoscelis adeliae)\nrow_number.10    PAL0708            10 Adelie Penguin (Pygoscelis adeliae)\nrow_number.100   PAL0809           100 Adelie Penguin (Pygoscelis adeliae)\nrow_number.101   PAL0910           101 Adelie Penguin (Pygoscelis adeliae)\nrow_number.102   PAL0910           102 Adelie Penguin (Pygoscelis adeliae)\nrow_number.103   PAL0910           103 Adelie Penguin (Pygoscelis adeliae)\n               Region    Island              Stage Individual.ID\nrow_number.1   Anvers Torgersen Adult, 1 Egg Stage          N1A1\nrow_number.10  Anvers Torgersen Adult, 1 Egg Stage          N5A2\nrow_number.100 Anvers     Dream Adult, 1 Egg Stage         N50A2\nrow_number.101 Anvers    Biscoe Adult, 1 Egg Stage         N47A1\nrow_number.102 Anvers    Biscoe Adult, 1 Egg Stage         N47A2\nrow_number.103 Anvers    Biscoe Adult, 1 Egg Stage         N49A1\n               Clutch.Completion   Date.Egg Culmen.Length..mm.\nrow_number.1                 Yes 2007-11-11               39.1\nrow_number.10                Yes 2007-11-09               42.0\nrow_number.100               Yes 2008-11-10               43.2\nrow_number.101               Yes 2009-11-09               35.0\nrow_number.102               Yes 2009-11-09               41.0\nrow_number.103               Yes 2009-11-15               37.7\n               Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\nrow_number.1                18.7                 181          3750   MALE\nrow_number.10               20.2                 190          4250   &lt;NA&gt;\nrow_number.100              18.5                 192          4100   MALE\nrow_number.101              17.9                 192          3725 FEMALE\nrow_number.102              20.0                 203          4725   MALE\nrow_number.103              16.0                 183          3075 FEMALE\n               Delta.15.N..o.oo. Delta.13.C..o.oo.\nrow_number.1                  NA                NA\nrow_number.10            9.13362         -25.09368\nrow_number.100           8.97025         -26.03679\nrow_number.101           8.84451         -26.28055\nrow_number.102           9.01079         -26.38085\nrow_number.103           9.21510         -26.22530\n                                           Comments\nrow_number.1         Not enough blood for isotopes.\nrow_number.10  No blood sample obtained for sexing.\nrow_number.100                                 &lt;NA&gt;\nrow_number.101                                 &lt;NA&gt;\nrow_number.102                                 &lt;NA&gt;\nrow_number.103                                 &lt;NA&gt;\n\n\nCan you figure out why the data.frame was sorted this way?\n\n\nSelecting columns of data\nSimilarly to how we were able to select rows of data from the data.frame, we can also select columns of the data.frame. First, to see what columns are in our data.frame we can use the colnames() function.\n\ncolnames(penguins)\n\n [1] \"studyName\"           \"Sample.Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual.ID\"       \"Clutch.Completion\"   \"Date.Egg\"           \n[10] \"Culmen.Length..mm.\"  \"Culmen.Depth..mm.\"   \"Flipper.Length..mm.\"\n[13] \"Body.Mass..g.\"       \"Sex\"                 \"Delta.15.N..o.oo.\"  \n[16] \"Delta.13.C..o.oo.\"   \"Comments\"           \n\n\nTo select columns, we can either specify the columns we want by name\n\nhead(penguins[, c(\"studyName\", \"Species\")])\n\n             studyName                             Species\nrow_number.1   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.2   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.3   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.4   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.5   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.6   PAL0708 Adelie Penguin (Pygoscelis adeliae)\n\n\nOr by the column index\n\nhead(penguins[, c(1, 3)])\n\n             studyName                             Species\nrow_number.1   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.2   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.3   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.4   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.5   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.6   PAL0708 Adelie Penguin (Pygoscelis adeliae)\n\n\nIf we want to rename a column, we can use the colnames() function to reassign the column name with a new name. First, we can get the index of the column we want to change. Then we can reassign that column with the new name.\n\n# This returns the index of the colnames() that matches \"Species\"\nwhich_col_to_change &lt;- which(colnames(penguins) == \"Species\")\n\n# Rename \"Species\" to \"species\"\ncolnames(penguins)[which_col_to_change] &lt;- \"species\"\n\nOne tricky aspect of base R subsetting on data.frames has to do with selecting a single column of data. If you select a single column of data from a data.frame what you get back is a vector. To ensure that you return a data.frame when selecting a single column, add the drop=FALSE argument when selecting a single column.\n\n# Returns a vector\nclass(penguins[, \"species\"])\n\n[1] \"character\"\n\n# Returns a data.frame\nclass(penguins[, \"species\", drop = FALSE])\n\n[1] \"data.frame\"\n\n\n\n\nModifying columns of data\nAdding or removing columns of data to a data.frame is as simple as specifying the name of the column you want to add along with the data that you want to add.\nFor example, if I wanted to convert the flipper length from mm to m I could multiply each value of flipper length by 0.001.\n\npenguins$flipper_length_m &lt;- penguins$Flipper.Length..mm. * 0.001\n\nThe above data transformation works because of R’s vectorization rule described above. What if I wanted to define a new variable based on a logical condition? For example, what if I wanted to create a new column based on whether or not a penguin was and Adelie or any other kind? A useful function for performing this action is ifelse(). ifelse() evaluates a logical condition and returns a value based on whether the condition evaluates to TRUE or FALSE.\n\npenguins$is_adelie &lt;- ifelse(\n  penguins$species == \"Adelie Penguin (Pygoscelis adeliae)\", \n  \"Adelie\", \n  \"Other\"\n  )\n\nIf I want to remove a column from the data.frame I can set the value of the column to NULL\n\npenguins$Flipper.Length..mm. &lt;- NULL",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#string-operations",
    "href": "r-basics.html#string-operations",
    "title": "R programming basics",
    "section": "String operations",
    "text": "String operations\nString operations are an important part of data cleaning. Base R supports many functions for transforming strings. Again, these string functions are typically vectorized meaning that they can easily be used to modify columns of data.frames.\nThe “species” column of the penguins data.frame is annoying to work with. We can simplify this column by creating simpler names for each species by excluding the scientific name and the word ‘penguin’ (we know they’re all penguins…)\nOne base R function we can use to find and replace text is called gsub(). gsub() takes what is called a regular expression to find text inside of a string and then replaces the text it finds with the text you supply. Regular expressions can become incredibly complex. With this complexity come a lot of power. It would be impossible to teach regular expression in this tutorial. Taking some time to understand the basics of regular expressions can go a long way during data cleaning.\n\n# Replace a space followed by the word 'penguins' and any other character\npenguins$species_clean &lt;- gsub(\n  pattern = \" penguin.*\", \n  replacement = \"\", \n  x = penguins$species, \n  ignore.case = TRUE\n  )\n\n# Show the counts for these new categories\ntable(penguins$species_clean)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nAnother base R function that is useful for extracting text is the substr() function. substr() extracts sub-strings from the supplied text based on the index of the text. For example, to create an even simpler representation of species we could extract only the first letter from the “species” column.\n\npenguins$species_simple &lt;- substr(penguins$species, start = 1, stop = 1)\ntable(penguins$species_simple)\n\n\n  A   C   G \n152  68 124 \n\n\nA slightly more complicated string operation is extracting text from a string. This involves two functions; one to find a match in the text and another to extract it. Let’s say we wanted to extract the word ‘penguin’ from each of the original species strings. We can do this with a combination of regmatches() and regexpr().\n\npenguins$only_penguin &lt;- regmatches(\n  x = penguins$species, \n  m = regexpr(pattern = \"penguin\", \n              text = penguins$species, \n              ignore.case = TRUE)\n  )\n\nR also supports basic operations on strings like making all characters upper or lowercase. For example, to ensure only_penguin contains ‘penguin’ (and not ‘Penguin’) we can convert the string to lowercase\n\n# lower case\nhead(tolower(penguins$only_penguin))\n\n[1] \"penguin\" \"penguin\" \"penguin\" \"penguin\" \"penguin\" \"penguin\"\n\n# or upper case\nhead(toupper(penguins$only_penguin))\n\n[1] \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\"\n\n\nAnother slightly complicated string operation is string splitting. String splitting in base R can be accomplished using the strsplit() function. This function is a little tricky because it returns a list. Here, we split the “species” column on every space character. Using the results from the strsplit() function most efficiently involves some advanced R skills.\n\nsplit_species &lt;- strsplit(\n  x = penguins$species, \n  split = \" \"\n  )\n\n# strsplit returns a list of character vectors split on every space\nhead(split_species)\n\n[[1]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[2]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[3]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[4]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[5]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[6]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n# To extract the species from this list, we can use a fancy trick\n# don't worry about what's happening here right now\npenguins$extracted_species &lt;- sapply(split_species, `[[`, 1)",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#control-flow",
    "href": "r-basics.html#control-flow",
    "title": "R programming basics",
    "section": "Control flow",
    "text": "Control flow\nR contains all of the typical control flow you expect from a programming language.\nFor-loops are somewhat under-utilized by many R users but can often be the most clear for new (and experienced) users. one common for-loop idion in R is to use a for-loop with the seq_along() function. seq_along() generates an index of the given vector that you can use to loop over.\n\nfor (i in seq_along(x)) {\n  doSomething(x[i])\n}\n\nOther looping operations in base R include while and repeat.\nConditional statements can be tested using if statements. These follow a familiar syntax\n\nx &lt;- 10\nif (x == 10) {\n  print(\"the value of x is 10\")\n} else {\n  print(\"the value of x is something else\")\n}\n\n[1] \"the value of x is 10\"\n\n\nIf statements in R are not vectorized. Use ifelse() to perform a vectorized if statement as demonstrated above.\nIf statements are often combined with for-loops.\n\n# Make some space to save the result\nresult &lt;- vector(\"character\", length = 10)\n\n# Determine if the number is even or odd\nfor (i in 1:10) {\n  if (i %% 2 == 0) {\n    result[i] &lt;- \"even\"\n  } else {\n    result[i] &lt;- \"odd\"\n  }\n}\n\nresult\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nR also contains a switch() statement which can be very useful when programming new functions.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functions",
    "href": "r-basics.html#functions",
    "title": "R programming basics",
    "section": "Functions",
    "text": "Functions\nEventually you’ll want to create your own functions to do stuff. Functions can be defined in R using the following syntax\n\nf &lt;- function(args) {\n  expr\n}\n\nTo make this a little more concrete, let’s define a function that classifies a penguin based on it’s flipper size. Our function will take in a flipper size and a threshold value for determining whether or not the penguin is “large” or “small” based on this flipper size. The function will return the resulting size designation.\n\ndetermine_size &lt;- function(flipper_size, threshold = 0.2) {\n  if (flipper_size &gt;= threshold) {\n    size_category &lt;- \"large\"\n  } else {\n    size_category &lt;- \"small\"\n  }\n  \n  return(size_category)\n}\n\n# Is a 0.5 meter flipper length large or small?\ndetermine_size(flipper_size = 0.5)\n\n[1] \"large\"",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functional-programming",
    "href": "r-basics.html#functional-programming",
    "title": "R programming basics",
    "section": "Functional programming",
    "text": "Functional programming\nFor-loops in R are very simple to understand but most R programmers use functions in the apply() family. These functions supply abstractions over common for-loops that make applying functions over lists much easier in R.\nThe most commonly used functional programming paradigm in R is the use of the lapply() function. The lapply() function takes a list as input, applies a function to each element of that list, and returns the results as a list. For example, above we split the penguins data.frame into a list of smaller data.frames for each species. Instead of repeating the code to create a histogram from each data.frame, let’s define a function that plots a histogram using a data.frame and then apply that function to every data.frame in our “by_species” list of data.frames\n\n# Define a function for plotting a histogram of flipper lengths\nplot_histogram &lt;- function(df) {\n  hist(df[, \"Flipper.Length..mm.\"], breaks=20)\n}\n\n# Apply this function to every data.frame\n# invisible() is used to hide the output from the hist() function\ninvisible(\n  lapply(X = by_species, FUN = plot_histogram)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR includes some other specialized functions for functional programming. Be sure to explore apply() for computing over rows/columns of matrices, sapply()and vapply() for applying functions over lists and returning vectors, mapply() for supplying multiple arguments to a list.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#resources",
    "href": "r-basics.html#resources",
    "title": "R programming basics",
    "section": "Resources",
    "text": "Resources\nThis only scratches the surface of programming in R. Check out these resources for more information.\n\nfasteR\nR for Data Science\nAdvanced R 2nd Edition\nAdvanced R 1st Edition\nThe R Inferno",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Effective data visualizations",
    "section": "",
    "text": "Plotting with ggplot2\nData visualization is one of the most important skills to develop as a data scientist. We use graphs, instead of tables for instance, to clearly communicate patterns, trends, and comparisons in data in a way that is inherently more interesting and informative than numbers in a grid.\nEdward Tufte’s, “The Visual Display of Quantitative Information”, is probably the most famous text in all of data visualization (I’m guessing). In this text, Tufte outlines a theory of graphics and provides extensive detail regarding techniques for displaying data that maximizes clarity, precision, and efficiency. Tufte describes graphical excellence by the following points:\nThese points describe how to make good visualizations. However, some visualizations are clearly better than others. Take a look at this presentation by Andrew Gelmen and Antony Unwin. Here, they describe what makes some visualizations effective vs. what makes others ineffective and distracting. I think the main conclusion comes down to design vs decoration. Effective data visualizations are intentionally designed to communicate a point in the clearest possible way. Ineffective visualizations often contain clutter in the form of decorations, unnecessary colors, patterns, lines, and unintuitive use of shapes and sizes.\nOutside of the design choices that go into creating effective visualizations, there are also other expectations required of graphs. The data points should be labelled so that the viewer immediately knows what they’re looking at. The axes should be labelled in a way that’s easy to read and do not distort the message. If using colors, the color palette should augment the visualization in a way that enhances the display of information and not only ‘looks pretty’.\nThere are probably thousands (millions?, infinite?) of types of visualizations in use today. However, in scientific communication, there are basically 5 types of graphs that are most commonly used; line graphs, bar graphs, histograms, scatter plots, and pie charts. Most of these have persisted in the literature (with the exception of pie charts) because of their ability to clearly and quickly display visual information about quantitative data. Many other forms of these basic charts exist primarily as variations on a theme but the core display of information remains the same.\nWe’ll use the ggplot2 R package to start creating effective visualizations. ggplot2 works in a way that can feel a little strange at first, especially if you’re used to creating plots in Python with matplotlib, for example. ggplot2 implements ideas from Leland Wilkinson’s, “Grammar of Graphics”. ggplot2, breaks down a graphic into several key components that can be combined in a layered fashion. These core components typically include:\nThe power of this approach lies in its declarative nature. You specify what you want to plot by defining these components, rather than detailing how to draw each element step-by-step (like in matplotlib). This makes it easier to build complex visualizations and to switch between different representations of the same data with minimal changes to the code.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#plotting-with-ggplot2",
    "href": "data-viz.html#plotting-with-ggplot2",
    "title": "Effective data visualizations",
    "section": "",
    "text": "Data: The dataset you want to visualize.\nAesthetics (aes): How columns in your data map to visual properties of the graphic. For example, mapping a ‘temperature’ column to the y-axis, a ‘time’ column to the x-axis, and a ‘city’ column to color.\nGeometries (geoms): The visual elements used to represent the data, such as points, lines, bars, histograms, etc.\nScales: Control how the aesthetic mappings translate to visual output (e.g., how data values are converted to colors, sizes, or positions on an axis).\nStatistics (stats): Transformations of the data that are performed before plotting (e.g., calculating a histogram, smoothing a line).\nCoordinates: The coordinate system used for the plot (e.g., Cartesian, polar).\nFaceting: Creating multiple subplots (a “trellis” or “lattice” plot) based on subsets of the data.\nTheme: Non-data elements like background, grid lines, and font choices.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#general-design-guidelines-for-plots",
    "href": "data-viz.html#general-design-guidelines-for-plots",
    "title": "Effective data visualizations",
    "section": "General design guidelines for plots",
    "text": "General design guidelines for plots\n\nDon’t use a special color for the background color. Use the same color as your presentation background. Generally, either white or black backgrounds are best\nAxes colors should be high contrast relative to the background. This means black axes on white backgrounds and white axes on black backgrounds.\nUse tick marks that have whole-number intervals. e.g. don’t use axis ticks that are 1.33, 2.75, 3.38, …. Instead, use numbers that people use naturally when counting, e.g. 1, 2, 3 or 2, 4, 6, etc.\nAxes need to be legible. The default font size is often too small. At least 12 point font.\nAxes labels should clearly communicate what is being plotted on each axis. Always show units.\nIf showing a plot title, use the title to tell the user what the conclusion is, not simply describing the data on each axis.\nIf using multiple colors, they should contrast well with each other. Warm colors (reds, yellows) can be used to emphasize selected data.\nKeep colors consistent across graphs. If you use the color “red” to represent treated samples in one plot, use the color “red” in all other plots where the treated samples are present.\nUse colors to communicate. Hues are not emotionally neutral. A good color palette can affect the audience’s perception of the figure (just like a bad one can).\nOnly use gridlines if you need them, most of the time they’re not needed.\nIf using gridlines, be judicious. Their color should be subtle and not obscure the data. Only use grids that matter for the data.\nChoose fonts that are easy to read. sans serif fonts are best. Helvetica or Arial are good default choices\nWhen drawing error bars, don’t make them so wide/large as to obscure the data",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#a-basic-ggplot",
    "href": "data-viz.html#a-basic-ggplot",
    "title": "Effective data visualizations",
    "section": "A basic ggplot",
    "text": "A basic ggplot\nggplot() constructs a plot from data, what it calls aesthetic mappings, and layers. Aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms. geoms then determine how the data is displayed. The other parts of the ggplot object have been handled automatically (i.e. scales, stats, coordinates, and theme). These, however, can be modified to enhance the plot. Check out the ggplot2 homepage for an overview or the ggplot2 book for details.\nThe code below demonstrates the most basic way of creating a plot with ggplot2.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis plot is okay but can be improved. Let’s improve this plot by removing the grey background and gridlines, increasing the font size of the axis ticks, improving the axis labels, and creating an informative title.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point(size = 2) +        # increase the size of the points\n  labs(                         # labs() can be used to modify axis label text\n    title = \"Fuel Efficiency Decreases with Increasing Weight\",    \n    x  = \"Weight (1000 lbs)\",\n    y = \"Fuel Efficiency (mpg)\") +\n  theme_classic() +             # Removes grey background and gridlines\n  theme(                        # Adjust the plot and axes titles, and text\n    plot.title = element_text(size = 18, face = \"bold\", color = \"black\"),\n    axis.title = element_text(size = 14, color = \"black\"),\n    axis.text = element_text(size = 12, color = \"black\")\n  )\n\n\n\n\n\n\n\n\n\nSaving themes\nIf you’re going to be using the same theme elements often, it can be helpful to save those as a new custom theme. You don’t have to understand exactly how this function works right now - simply modify the arguments to the theme() function and figure this out as you improve\n\ntheme_clean &lt;- function(...) {\n  ggplot2::theme_classic(...) %+replace%\n  ggplot2::theme(\n      text = ggplot2::element_text(family = \"Helvetica\"),\n      plot.title = ggplot2::element_text(size = 18, face = \"bold\", color = \"black\", hjust = 0),\n      axis.title = ggplot2::element_text(size = 14, color = \"black\"),\n      axis.text = ggplot2::element_text(size = 12, color = \"black\")\n    )\n}\n\n# The new theme can be applied to other plots\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point() +        \n  theme_clean()\n\n\n\n\n\n\n\n\nThis plot is contains the same information but more quickly and clearly communicates the message by just modifying the design. What other element could be added to this plot to make the trend more apparent?",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#scatter-plots",
    "href": "data-viz.html#scatter-plots",
    "title": "Effective data visualizations",
    "section": "Scatter plots",
    "text": "Scatter plots\nWe already showed a basic example of creating a scatter plot above. You can use that as a starting point for generating scatter plots. However, one common issue when designing scatter plots is overplotting, or, showing so many points that the data is cluttered. Below is an example of overplotting.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  theme_clean()\n\n\n\n\n\n\n\n\nOne technique to overcome overplotting is to add transparency to the points\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(alpha = 0.05) +\n  theme_clean()\n\n\n\n\n\n\n\n\nAnother is to change the point type. Here, we plot each point as a single dot\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(shape = \".\") +\n  theme_clean()\n\n\n\n\n\n\n\n\nAnd another is random subsampling.\n\nrandom_rows &lt;- sample.int(nrow(diamonds), size = 500)\n\nggplot(diamonds[random_rows, ], aes(carat, price)) +\n  geom_point() +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe density of the points could also be summarized. For example, explore geom_hex() or geom_density2d() geoms.\nLet’s make a final version of this plot by cleaning up the background, axes, and titles. We’ll add a trendline to emphasize the relationship and we’ll also use a function to transform the y-axis to dollar format.\n\nggplot(diamonds[random_rows, ], aes(carat, price)) +\n  geom_point() +\n  geom_smooth(se = FALSE, color = \"red\") +   # Adds a smooth trendline\n  labs(\n    title = \"Larger Diamonds are More Expensive\",    \n    x  = \"Carat\",\n    y = \"Price ($)\") +\n  scale_y_continuous(             # Formats the y-axis labels as dollar amounts\n    labels = scales::dollar_format()\n    )  +\n  theme_clean()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#line-graphs",
    "href": "data-viz.html#line-graphs",
    "title": "Effective data visualizations",
    "section": "Line graphs",
    "text": "Line graphs\nLine graphs are meant to emphasize change in the y-variable over the x-variable. When designing line graphs:\n\nTake advantage the range of the data. Data should take up about 3/4 of the y-axis. ggplot has pretty good defaults for this automatically.\nChoose line weights that do not overshadow points (if present)\nDashed lines can be hard to read. Use contrasting colors instead\nIf presenting two graphs next to each other be sure to match the axes ranges\n\n\nggplot(economics, aes(date, unemploy)) + \n  geom_line() +\n  labs(\n    title = \"Unemployed Individuals Over Time\",\n    x = \"Year\", \n    y = \"Thousands of Persons\"\n    ) +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  theme_clean()\n\n\n\n\n\n\n\n\nWe can also explore adding multiple colors to the plot to compare values. Another useful technique is to add the legend to the plot area.\n\necon_2 &lt;- economics_long[!economics_long$variable %in% c(\"pce\", \"pop\"), ]\n\nggplot(econ_2, aes(date, value01, colour = variable)) +\n  geom_line() +\n  labs(\n    x = \"Year\", \n    y = \"Variable\",\n    color = NULL\n    ) +\n  scale_color_brewer(                             # Add better colors and labels\n    palette = \"Set1\", \n    breaks = c(\"psavert\", \"uempmed\", \"unemploy\"),\n    labels = c(\"Savings\", \"Duration\", \"Unemployed\")\n    ) +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  guides(color = guide_legend(position = \"inside\")) +  # add the legend inside the plot\n  theme_clean() +\n  theme(\n    legend.position.inside = c(0.5, 0.85),      # specify where to put the legend\n    legend.text = element_text(size = 12)\n    )",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#bar-graphs",
    "href": "data-viz.html#bar-graphs",
    "title": "Effective data visualizations",
    "section": "Bar graphs",
    "text": "Bar graphs\nPeople generally use bar graphs to display a mean and some variation around the mean. Filled bar plots can also be used to show proportions.\n\nIf there are relatively few data points, consider showing all points and a crossbar for the mean and error instead. For example, as a beeswarm plot. If there are many points, consider a boxplot.\nIn most cases, it’s usually best to start the y-axis at 0 if the data has a natural 0. The exception is for charts where 0 does not indicate ‘nothing’ but rather exists on a continuum, for example, temperature in Fahrenheit.\nDon’t have bars with too thick of outlines\nAvoid bars that are too thin. Aim for about 1/3 width of the bar as space between bars\nPlace extra space between categories if showing bars next to each other\nIf showing statistical information try not to overwhelm the data. Use subtle thin lines for comparisons and asterisks for significance.\n\nActivity: Make this example plot better.\n\ndf &lt;- data.frame(trt = c(\"a\", \"b\", \"c\"), outcome = c(2.3, 1.9, 3.2))\n\nggplot(df, aes(trt, outcome)) +\n  geom_col() +\n  theme_clean()",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#histograms",
    "href": "data-viz.html#histograms",
    "title": "Effective data visualizations",
    "section": "Histograms",
    "text": "Histograms\nHistograms are used to show distributions of data with their relative frequencies.\n\nThe number of bins influences the interpretation of the data. Play around with the number of bins to ensure the best display\nDon’t assign different colors to different bins - it doesn’t add any information\nDon’t add spaces between bins, as in a bar plot\nIf displaying two datasets, you can either overlay each with a different fill and some transparency or split into two histograms with the same y-axis.\n\nActivity: Make this example plot better.\n\nggplot(diamonds, aes(carat)) +\n  geom_histogram() +\n  theme_clean()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#saving-images",
    "href": "data-viz.html#saving-images",
    "title": "Effective data visualizations",
    "section": "Saving images",
    "text": "Saving images\nUsing ggplot2 you can save images with the ggsave() function. ggsave() can automatically detect the image format by the file extension. The ggsave() function works by saving the last plot created.\n\nggplot(data, aes(x, y)) + \n  geom_point()\n\nggsave(\"my-pretty-plot.pdf\", width = 8, height = 6)\n\nIf using base R, you typically open the graphics device (png(), pdf(), jpeg(), etc.) first, depending on the file format you want to save, then create the plot, and close the plot device.\n\npdf(\"another-pretty-plot.pdf\", width = 8, height = 6)\nplot(x, y)\ndev.off()",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#image-file-formats",
    "href": "data-viz.html#image-file-formats",
    "title": "Effective data visualizations",
    "section": "Image file formats",
    "text": "Image file formats\n\nSave your images in .pdf format or .svg format. These formats are called vector graphics. Vector graphics are infinitely scalable. They never lose resolution no matter the zoom level.\nVector graphics don’t play nicely with Word documents. If you need to share images that will be used in Word docs, save your image as a high-quality .png. .png files are raster graphics. Raster based images store actual pixel values so they cannot be infinitely zoomed. They lose quality at high magnification. Consider at least a dpi of 300 when saving .pngs.\nVector graphics are also editable using a program like Inkscape. Often, you may need to add some custom color or annotations using an illustrator program. Vector graphics allow you to do this.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#resources",
    "href": "data-viz.html#resources",
    "title": "Effective data visualizations",
    "section": "Resources",
    "text": "Resources\n\nLearning ggplot\nggplot2 Book\nModern Data Visualization with R\nR for Data Science Data Viz chapter\nFundamentals of Data Visualization\nThe Visual Display of Quantitative Information\nR Graph Gallery",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "presenting-science.html",
    "href": "presenting-science.html",
    "title": "Scientific presentations",
    "section": "",
    "text": "General tips\nA science presentation is more than just displaying your results on a slide deck. Good content doesn’t speak for itself. A good science presentation is more like a story that weaves together a problem (gap in knowledge), a logical thought process for solving that problem (hypothesis), data that supports your claims (figures), and a compelling delivery. The structure of your presentation, the design of the slides, the flow of information from one slide to the next, and your delivery of that information, are all aspects that your need to consider if you want to create informative and interesting presentations. Like everything else in life, this takes time and practice. Below are a few tips that I have learned. For a good book on this topic check out Designing Science Presentations by Matt Carter.",
    "crumbs": [
      "Scientific presentations"
    ]
  },
  {
    "objectID": "presenting-science.html#general-tips",
    "href": "presenting-science.html#general-tips",
    "title": "Scientific presentations",
    "section": "",
    "text": "Keep it simple. Well designed presentations translate complex information into simple messages. Richard Feyman was famous for this ability.\nSubtract! It’s easy to keep adding information, text, images, etc. to a presentation. It’s much harder to know what information to take away. Generally, try to deliberately take away any elements of your presentation that don’t add value. Simple is not boring, it’s focused.\nYour presentation is about the audience. Determine who your audience is and what they are likely to know or not know. Design all aspects of your presentation with this in mind.\nLearn some principles of design and apply them to your presentations. Design is not decoration. Collect examples of good presentations and take notes from people who do it well.\nColor is a powerful tool for communication. Intentional color choices can draw attention to important data, emphasize points, and evoke emotions. Poorly chosen colors can be distracting and make your content unreadable\nFont choices matter. Fonts can convey tone and personality. Use a font that increases readability. Use italics or bolding to emphasize important ideas.\nWords matter. Choose words that express your ideas precisely, concisely, and as clearly as possible.\nThere’s no correct number of slides. It comes down to your presentation and delivery.\nEverything doesn’t need to be shown - sometimes the best way of communicating an idea is by narration alone.",
    "crumbs": [
      "Scientific presentations"
    ]
  },
  {
    "objectID": "presenting-science.html#slide-presentation-tips",
    "href": "presenting-science.html#slide-presentation-tips",
    "title": "Scientific presentations",
    "section": "Slide presentation tips",
    "text": "Slide presentation tips\nGenerally, you’ll want to design a slide presentation in a way that focuses on overall ideas rather than just moving from one slide to the next. Think about the structure of your presentation. This generally means following the scientific method.\nFor your internship presentations, the structure can be something like:\n\nIntroduction / background\n\nWhat is the context and why should we care about it?.\n\nWhat does the audience need to know in order to understand your conclusions later?\nImportantly, lead the audience in understanding the gap in knowledge. What problem are you trying to solve?. Clearly emphasize your goal.\nThink about the introduction like a funnel. Start with a broad topic and then narrow in on your what your focus will be for the remainder of the presentation. General –&gt; Specific.\n\nExperimental methods\n\nClearly explain how you’re going to solve the problem. What data you used, and what methods.\nExplain why you chose the methods you did and how they answer parts of your overall hypothesis.\nDon’t forget to mention the things that your methods lack.\n\nResults\n\nTake the audience through your data step-by-step, starting with the simplest results and building into more complex ideas.\nDon’t show too much on one slide. One (or two) figures is usually all you need.\nDon’t put anything on a slide that you aren’t going to discuss.\nExplain your figures if you put it on the slide, explain it. Tell the audience what the graph is showing, explain to them the conclusions you draw from it.\nBe sure to connect your data to the larger story of your presentation - how does this data relate to the information you talked about earlier?\n\nDiscussion / conclusions\n\nSpecific –&gt; General\nRecap the original problem.\nRecap your methods and why they are appropriate to solve this problem.\nPull out the main points and emphasize them. Tell the audience why they are important.\nMention any gaps or future work.\nConclude on broader implications.\n\n\nIn addition to this structure, sometimes it’s good to include some measure of progress in the form of slide numbers or a “home” slide - a slide that you come back to that is used to orient the audience to their place in the presentation.\n\nExample\nHere is a link to my PhD proposal slides presentation. I tried to incorporate good design principles when creating this. While it’s certainly not perfect and I’m certainly not the best presenter, here are a few of the design choices I tried to make to improve the presentation.\n\nslide 1: eye-catching background with presenter information in contrasting white font. Note that the same font is used throughout the presentation.\nslide 2: High contrast slides, modern black font (Helvetica Neue) on white inconspicuous background. Relatively few words on the slide. Give the audience an idea of the structure of the presentation.\nslide 3: Section header slide used to orient the audience. Section headers can be a good way to give structure to a presentation.\nslides 4-8: General information to specific problem. What are TEs? Why are they important for cancer? How do they contribute to immunotherapy response? Why is this difficult to study? General –&gt; Specific.\nslide 9: An entire slide dedicated to emphasizing the main point of the rest of the proposal.\nslide 11: The use of bolding in the bullet points to emphasize the main point of each. Layout of the slide is designed as question on the left and answer on the right in italics.\nslide 17-16: Results slides showing limited number of figures on each slide with clear data and axes labels. Each title reflects what I want the audience to take away from the figures.\nslide 22: Use color to emphasize the main paper of interest.\nslide 23: Repeat the question and answer structure\nslide 39: Conclusions recap the entire proposal. Bold text is used to emphasize where this proposal falls into the overall scientific goals.",
    "crumbs": [
      "Scientific presentations"
    ]
  },
  {
    "objectID": "summarized-experiments.html",
    "href": "summarized-experiments.html",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\nThe SummarizedExperiment object coordinates four main parts:\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#what-are-summarizedexperiments",
    "href": "summarized-experiments.html#what-are-summarizedexperiments",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "assay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [&lt;- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\n\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts &lt;- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1      43     195     205      55      73\nGene2      37     102      23     130      36\nGene3      73      60      33      46      34\nGene4     119     123     346      26     149\nGene5     121      38     230     132     317\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata &lt;- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control  32\nSample2    Sample2   Control  96\nSample3    Sample3   Control  41\nSample4    Sample4 Treatment  48\nSample5    Sample5 Treatment  52\nSample6    Sample6 Treatment   2\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges &lt;- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) &lt;- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 257333-257432      - |       ID001 protein_coding\n    Gene2     chr1 543028-543127      + |       ID002 protein_coding\n    Gene3     chr1 463891-463990      + |       ID003 protein_coding\n    Gene4     chr1 679135-679234      + |       ID004         lncRNA\n    Gene5     chr1 829784-829883      - |       ID005 protein_coding\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 723767-723866      - |       ID196         lncRNA\n  Gene197     chr2 673554-673653      + |       ID197 repeat_element\n  Gene198     chr2 593478-593577      - |       ID198 repeat_element\n  Gene199     chr2 543982-544081      + |       ID199         lncRNA\n  Gene200     chr2 114505-114604      - |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse &lt;- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |&gt; head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1      43     195     205      55      73     105\nGene2      37     102      23     130      36      91\nGene3      73      60      33      46      34     105\nGene4     119     123     346      26     149      52\nGene5     121      38     230     132     317      63\nGene6      14      30      73      45     121      86\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt;\nSample1     Sample1 Control          32\nSample2     Sample2 Control          96\nSample3     Sample3 Control          41\nSample4     Sample4 Treatment        48\nSample5     Sample5 Treatment        52\nSample6     Sample6 Treatment         2\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 257333-257432      - |       ID001 protein_coding\n    Gene2     chr1 543028-543127      + |       ID002 protein_coding\n    Gene3     chr1 463891-463990      + |       ID003 protein_coding\n    Gene4     chr1 679135-679234      + |       ID004         lncRNA\n    Gene5     chr1 829784-829883      - |       ID005 protein_coding\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 723767-723866      - |       ID196         lncRNA\n  Gene197     chr2 673554-673653      + |       ID197 repeat_element\n  Gene198     chr2 593478-593577      - |       ID198 repeat_element\n  Gene199     chr2 543982-544081      + |       ID199         lncRNA\n  Gene200     chr2 114505-114604      - |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        &lt;character&gt;    &lt;character&gt;\nGene1         ID001 protein_coding\nGene2         ID002 protein_coding\nGene3         ID003 protein_coding\nGene4         ID004         lncRNA\nGene5         ID005 protein_coding\n...             ...            ...\nGene196       ID196         lncRNA\nGene197       ID197 repeat_element\nGene198       ID198 repeat_element\nGene199       ID199         lncRNA\nGene200       ID200 protein_coding",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "href": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts &lt;- assay(se, \"counts\")\ncpm &lt;- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") &lt;- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") &lt;- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") &lt;- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1] 32 96 41 48 52  2\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch &lt;- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt; &lt;factor&gt;\nSample1     Sample1 Control          32        A\nSample2     Sample2 Control          96        B\nSample3     Sample3 Control          41        C\nSample4     Sample4 Treatment        48        A\nSample5     Sample5 Treatment        52        B\nSample6     Sample6 Treatment         2        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep &lt;- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene3         ID003 protein_coding      TRUE\nGene4         ID004         lncRNA     FALSE\nGene5         ID005 protein_coding      TRUE\n...             ...            ...       ...\nGene196       ID196         lncRNA     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199         lncRNA     FALSE\nGene200       ID200 protein_coding      TRUE",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "href": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt &lt;- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |&gt; head()\n\n       Sample4  Sample5  Sample6\nGene1 11.44995 11.80667 12.28449\nGene2 12.58495 10.90951 12.07571\nGene3 11.14041 10.65770 12.38284\nGene4 10.44002 12.78708 11.26302\nGene5 12.61464 13.97693 11.59412\nGene6 11.05977 12.48145 12.16585\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age &gt; 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample2 Sample3 Sample5\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding &lt;- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 61 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(61): Gene1 Gene2 ... Gene194 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 61 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene3         ID003 protein_coding      TRUE\nGene5         ID005 protein_coding      TRUE\nGene13        ID013 protein_coding      TRUE\n...             ...            ...       ...\nGene188       ID188 protein_coding      TRUE\nGene191       ID191 protein_coding      TRUE\nGene193       ID193 protein_coding      TRUE\nGene194       ID194 protein_coding      TRUE\nGene200       ID200 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |&gt; head()\n\n        Sample1  Sample2   Sample3  Sample4   Sample5  Sample6\nGene1  2187.182 9569.143  9740.105 2797.558  3582.295 4988.835\nGene2  1748.665 5449.882  1091.030 6143.958  1923.488 4316.683\nGene3  3582.295 2850.763  1678.535 2257.336  1615.432 5340.793\nGene5  5749.038 1932.859 11286.682 6271.678 16124.110 3091.569\nGene13 1678.535 5152.616  1140.305 5645.982  4858.180 2043.047\nGene15 2748.062 7459.495  4323.499 3484.150  3658.479 3102.747\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi &lt;- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 &lt;- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene51        ID051 protein_coding      TRUE\nGene52        ID052 protein_coding      TRUE\nGene53        ID053 repeat_element     FALSE\nGene54        ID054 repeat_element     FALSE\nGene55        ID055 protein_coding      TRUE\n...             ...            ...       ...\nGene196       ID196         lncRNA     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199         lncRNA     FALSE\nGene200       ID200 protein_coding      TRUE\n\n\n\nassay(chr2, \"counts\") |&gt; head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51     231     173      57      66     138      48\nGene52      40     120      60     126     223      80\nGene53      47     155     109      54     138      48\nGene54     107      95      93     120     134     198\nGene55      89      33      87      67     171      85\nGene56      35      65      96      61     160      95\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\n   Gene51     chr2 199333-199432      + |       ID051 protein_coding      TRUE\n   Gene52     chr2 405579-405678      + |       ID052 protein_coding      TRUE\n   Gene53     chr2 753001-753100      - |       ID053 repeat_element     FALSE\n   Gene54     chr2 644644-644743      + |       ID054 repeat_element     FALSE\n   Gene55     chr2 115245-115344      - |       ID055 protein_coding      TRUE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 723767-723866      - |       ID196         lncRNA     FALSE\n  Gene197     chr2 673554-673653      + |       ID197 repeat_element     FALSE\n  Gene198     chr2 593478-593577      - |       ID198 repeat_element     FALSE\n  Gene199     chr2 543982-544081      + |       ID199         lncRNA     FALSE\n  Gene200     chr2 114505-114604      - |       ID200 protein_coding      TRUE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected &lt;- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 62 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(62): Gene8 Gene9 ... Gene197 Gene198\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#saving-a-summarizedexperiment",
    "href": "summarized-experiments.html#saving-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse &lt;- readRDS(\"/path/to/se.rds\")",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "href": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") &lt;- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info &lt;- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\"",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#closing-thoughts",
    "href": "summarized-experiments.html#closing-thoughts",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html",
    "href": "differential-expression-analysis.html",
    "title": "Differential Expression Analysis",
    "section": "",
    "text": "Overview\nThis vignette outlines some common steps for RNA-seq analysis highlighting functions present in the coriell package. To illustrate some of the analysis steps I will borrow examples and data from the rnaseqGene and RNAseq123 Bioconductor workflows. Please check out the above workflows for more details regarding RNA-seq analysis.\nThis vignette contains my opinionated notes on performing RNA-seq analyses. I try to closely follow best practices from package authors but if any information is out of date or incorrect, please let me know.\nDifferential gene expression analysis using RNA-seq typically consists of several steps:",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#overview",
    "href": "differential-expression-analysis.html#overview",
    "title": "Differential Expression Analysis",
    "section": "",
    "text": "Quality control of the fastq files with a tool like FastQC or fastp\nAlignment of fastq files to a reference genome using a splice-aware aligner like STAR or transcript quantification using a pseudoaligner like Salmon.\nIf using a genome aligner, read counting with Rsubread::featureCounts or HTSeq count to generate gene counts. If using a transcript aligner, importing gene-level counts using the appropriate offsets with tximport or tximeta\nQuality control plots of the count level data including PCA, heatmaps, relative-log expression boxplots, density plots of count distributions, and parallel coordinate plots of libraries. Additionally, check the assumptions of global scaling normalization.\nDifferential expression testing on the raw counts using edgeR, DESeq2, baySeq, or limma::voom\nCreation of results plots such as volcano or MA plots.\nGene ontology analysis of interesting genes.\nGene set enrichment analysis.",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#quality-control",
    "href": "differential-expression-analysis.html#quality-control",
    "title": "Differential Expression Analysis",
    "section": "Quality Control",
    "text": "Quality Control\nfastp has quickly become my favorite tool for QC’ing fastq files primarily because it is fast and produces nice looking output files that are also amenable to summarization with MultiQC. fastp can also perform adapter trimming on paired-end reads. I tend to fall in the camp that believes read quality trimming is not necessary for RNA-seq alignment. However, I have never experienced worse results after using the adapter trimming with fastp so I leave it be and just inspect the output carefully.\nA simple bash script for running fastp over a set of fastq files might look something like this:\n\n#!/usr/bin/env bash\n#\n# Run fastp on the raw fastq files\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nFQ=/path/to/00_fastq       # Directory containing raw fastq files\nSAMPLES=sample-names.txt   # A text file listing basenames of fastq files\nOUT=/path/to/put/01_fastp  # Where to save the fastp results\nTHREADS=8\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    fastp -i $FQ/${SAMPLE}_R1.fq.gz \\\n          -I $FQ/${SAMPLE}_R2.fq.gz \\\n          -o $OUT/${SAMPLE}.trimmed.1.fq.gz \\\n          -O $OUT/${SAMPLE}.trimmed.2.fq.gz \\\n          -h $OUT/${SAMPLE}.fastp.html \\\n          -j $OUT/${SAMPLE}.fastp.json \\\n          --detect_adapter_for_pe \\\n          -w $THREADS\ndone\n\nWhere sample-names.txt is a simple text file with each basename like so:\nControl1\nControl2\nControl3\nTreatment1\nTreatment2\nTreatment3\nIt is important to name the results files with *.fastp.{json|html} so that multiqc can recognize the extensions and combine the results automatically.",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#alignment-and-quantification",
    "href": "differential-expression-analysis.html#alignment-and-quantification",
    "title": "Differential Expression Analysis",
    "section": "Alignment and Quantification",
    "text": "Alignment and Quantification\n\nSalmon\nI tend to perform quantification with Salmon to obtain transcript-level counts for each sample. A simple bash script for performing quantification with Salmon looks like:\n\n#!/usr/bin/env bash\n#\n# Perform transcript quantification with Salmon\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nSAMPLES=sample-names.txt  # Same sample-names.txt file as above  \nIDX=/path/to/salmon-idx   # Index used by Salmon\nFQ=/path/to/01_fastp      # Directory containing the fastp output\nOUT=/path/to/02_quants    # Where to save the Salmon results\nTHREADS=12\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    salmon quant \\\n           -i $IDX \\\n           -l A \\\n           -1 $FQ/${SAMPLE}.trimmed.1.fq.gz \\\n           -2 $FQ/${SAMPLE}.trimmed.2.fq.gz \\\n           --gcBias \\\n           --seqBias \\\n           --threads $THREADS \\\n           -o $OUT/${SAMPLE}_quants\ndone\n\nI tend to always use the --gcBias and --seqBias flags as they don’t impair accuracy in the absence of biases (quantification just takes a little longer).\n\n\nSTAR\nSometimes I also need to produce genomic coordinates for alignments. For this purpose I tend to use STAR to generate BAM files as well as produce gene-level counts with it’s inbuilt HTSeq-count functionality. A simple bash script for running STAR might look like:\n\n#!/usr/bin/env bash\n#\n# Align reads with STAR\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nSAMPLES=sample-names.txt   # Same sample-names.txt file as above\nFQ=/path/to/01_fastp       # Directory containing the fastp output\nOUT=/path/to/03_STAR_outs  # Where to save the STAR results\nIDX=/path/to/STAR-idx      # Index used by STAR for alignment\nTHREADS=24\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n  STAR --runThreadN $THREADS \\\n       --genomeDir $IDX \\\n       --readFilesIn ${FQ}/${SAMPLE}.trimmed.1.fq.gz ${FQ}/${SAMPLE}.trimmed.2.fq.gz \\\n       --readFilesCommand zcat \\\n       --outFilterType BySJout \\\n       --outFileNamePrefix ${OUT}/${SAMPLE}_ \\\n       --alignSJoverhangMin 8 \\\n       --alignSJDBoverhangMin 1 \\\n       --outFilterMismatchNmax 999 \\\n       --outFilterMismatchNoverReadLmax 0.04 \\\n       --alignIntronMin 20 \\\n       --alignIntronMax 1000000 \\\n       --alignMatesGapMax 1000000 \\\n       --outMultimapperOrder Random \\\n       --outSAMtype BAM SortedByCoordinate \\\n       --quantMode GeneCounts;\ndone\n\nFor STAR I tend to use the ENCODE default parameters above for human samples and also output gene level counts using the --quantMode GeneCounts flag.",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#generating-a-matrix-of-gene-counts",
    "href": "differential-expression-analysis.html#generating-a-matrix-of-gene-counts",
    "title": "Differential Expression Analysis",
    "section": "Generating a matrix of gene counts",
    "text": "Generating a matrix of gene counts\nThe recommended methods for performing differential expression analysis implemented in edgeR, DESeq2, baySeq, and limma::voom all require raw count matrices as input data.\n\nImporting transcript-level counts from Salmon\nWe use R to import the quant files into the active session. tximeta will download the appropriate metadata for the reference genome used and import the results as a SummarizedExperiment object. Check out the tutorial for working with SummarizedExperiment objects if you are unfamiliar with their structure.\nThe code below will create a data.frame mapping sample names to file paths containing quantification results. This data.frame is then used by tximeta to import Salmon quantification results at the transcript level (along with transcript annotations). Then, we use summarizeToGene() to summarize the tx counts to the gene level. Finally, we transform the SummarizedExperiment object to a DGEList for use in downstream analysis with edgeR\n\nlibrary(tximeta)\nlibrary(edgeR)\n\n\nquant_files &lt;- list.files(\n  path = \"02_quants\",\n  pattern = \"quant.sf\",\n  full.names = TRUE,\n  recursive = TRUE\n)\n\n# Extract samples names from filepaths\nnames(quant_files) &lt;- gsub(\"02_quants\", \"\", quant_files, fixed = TRUE)\nnames(quant_files) &lt;- gsub(\"_quants/quant.sf\", \"\", names(quant_files), fixed = TRUE)\n\n# Create metadata for import\ncoldata &lt;- data.frame(\n  names = names(quant_files), \n  files = quant_files,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = 3))\n  )\n\n# Import transcript counts with tximeta\nse &lt;- tximeta(coldata)\n\n# Summarize tx counts to the gene-level\ngse &lt;- summarizeToGene(se, countsFromAbundance = \"scaledTPM\")\n\n# Import into edgeR for downstream analysis\ny &lt;- SE2DGEList(gse)\n\n\n\nImporting gene counts from STAR\nIf you used STAR to generate counts with HTSeq-count then edgeR can directly import the results for downstream analysis like so:\n\nlibrary(edgeR)\n\n\n# Specify the filepaths to gene counts from STAR\ncount_files &lt;- list.files(\n  path = \"03_STAR_outs\", \n  pattern = \"*.ReadsPerGene.out.tab\", \n  full.names = TRUE\n  )\n\n# Name the file with their sample names\nnames(count_files) &lt;- gsub(\".ReadsPerGene.out.tab\", \"\", basename(count_files))\n\n# Import HTSeq counts into a DGEList \ny &lt;- readDGE(\n  files = count_files, \n  columns = c(1, 2),  # Gene name and 'unstranded' count columns\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = 3)),\n  labels = names(count_files)\n  )\n\n\n\nCounting reads with featureCounts()\nAnother (preferred) option for generating counts from BAM files is to use the function featureCounts() from the Rsubread R package. featureCounts() is very fast and has many options to control exactly how reads are counted. featureCounts() is also very general. For example, you can use featureCounts() to count reads over exons, introns, or arbitrary genomic ranges - it’s a very useful tool.\nfeatureCounts() can use an inbuilt annotation or take a user supplied GTF file to count reads over. The inbuilt annotation (NCBI RefSeq) works particularly well with downstream functions implemented in edgeR. See ?Rsubread::featureCounts() for more information about using your own GTF file.\nThe resulting object can also be easily coerced to a DGEList for downstream analysis.\n\nlibrary(Rsubread)\n\n\n# Specify the path to the BAM files produced by STAR\nbam_files &lt;- list.files(\n  path = \"path/to/bam_files\", \n  pattern = \"*.bam$\", \n  full.names = TRUE\n  )\n\n# Optionally name the bam files\nnames(bam_files) &lt;- gsub(\".bam\", \"\", basename(bam_files))\n\n# Count reads over genes for a paired-end library \nfc &lt;- featureCounts(\n  files = bam_files,\n  annot.inbuilt = \"hg38\",\n  isPairedEnd = TRUE,\n  nThreads = 12\n)\n\n# Coerce to DGEList for downstream analysis\ny &lt;- edgeR::featureCounts2DGEList(fc)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#test-data",
    "href": "differential-expression-analysis.html#test-data",
    "title": "Differential Expression Analysis",
    "section": "Test data",
    "text": "Test data\nWe will use data from the airway package to illustrate differential expression analysis steps. Please see Section 2 of the rnaseqGene workflow for more information.\nBelow, we load the data from the airway package and use SE2DGEList to convert the object to a DGElist for use with edgeR.\n\nlibrary(airway)\nlibrary(edgeR)\n\n\n# Load the SummarizedExperiment object\ndata(airway)\n\n# Set the group levels\nairway$group &lt;- factor(airway$dex, levels = c(\"untrt\", \"trt\"))\n\n# Convert to a DGEList to be consistent with above steps\ny &lt;- SE2DGEList(airway)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#library-qc",
    "href": "differential-expression-analysis.html#library-qc",
    "title": "Differential Expression Analysis",
    "section": "Library QC",
    "text": "Library QC\nBefore we perform differential expression analysis it is important to explore the samples’ library distributions to ensure good quality before downstream analysis. There are several diagnostic plots we can use for this purpose implemented in the coriell package. However, first we must remove any features that have too low of counts for meaningful differential expression analysis. This can be achieved using edgeR::filterByExpr().\n\n# Determine which genes have enough counts to keep around\nkeep &lt;- filterByExpr(y)\n\n# Remove the unexpressed genes\ny &lt;- y[keep,,keep.lib.sizes = FALSE]\n\nAt this stage it is often wise to perform library QC on the library size normalized counts. This will give us an idea about potential global expression differences and potential outliers before introducing normalization factors. We can use edgeR to generate log2 counts-per-million values for the retained genes.\n\nlogcounts &lt;- cpm(y, log = TRUE)\n\n\nRelative log-expression boxplots\nThe first diagnostic plot we can look at is a plot of the relative log expression values. RLE plots are good diagnostic tools for evaluating unwanted variation in libraries.\n\nlibrary(ggplot2)\nlibrary(coriell)\n\n\nplot_boxplot(logcounts, metadata = y$samples, fillBy = \"group\", \n             rle = TRUE, outliers = FALSE) +\n  labs(title = \"Relative Log Expression\",\n       x = NULL,\n       y = \"RLE\",\n       color = \"Treatment Group\")\n\n\n\n\n\n\n\n\nWe can see from the above RLE plot that the samples are centered around zero and have mostly similar distributions. It is also clear that two of the samples, “SRR1039520” and “SRR1039521”, have slightly different distributions than the others.\n\n\nLibrary density plots\nLibrary density plots show the density of reads corresponding to a particular magnitude of counts. Shifts of these curves should align with group differences and generally samples from the same group should have overlapping density curves\n\nplot_density(logcounts, metadata = y$samples, colBy = \"group\") +\n  labs(title = \"Library Densities\",\n       x = \"logCPM\",\n       y = \"Density\",\n       color = \"Treatment Group\")\n\n\n\n\n\n\n\n\n\n\nSample vs Sample Distances\nWe can also calculate the euclidean distance between all pairs of samples and display this on a heatmap. Again, samples from the same group should show smaller distances than sample pairs from differing groups.\n\nplot_dist(logcounts, metadata = y$samples[, \"group\", drop = FALSE])\n\n\n\n\n\n\n\n\n\n\nParallel coordinates plot\nParallel coordinates plots are useful for giving you an idea of how the most variable genes change between treatment groups. These plots show the expression of each gene as a line on the y-axis traced between samples on the x-axis.\n\nplot_parallel(logcounts, y$samples, colBy = \"group\", \n              removeVar = 0.9, alpha = 0.05) +\n  labs(title = \"10% Most Variable Genes\",\n       x = \"Sample\",\n       y = \"logCPM\",\n       color = \"Treatment Group\")\n\nRemoving 90% lowest variance features...\n\n\n\n\n\n\n\n\n\n\n\nCorrelations between samples\nWe can also plot the pairwise correlations between all samples. These plots can be useful for identifying technical replicates that deviate from the group\n\nplot_cor_pairs(logcounts, cex_labels = 1)\n\n\n\n\n\n\n\n\n\n\nPCA\nPrincipal components analysis is an unsupervised method for reducing the dimensionality of a dataset while maintaining its fundamental structure. PCA biplots can be used to examine sample groupings following PCA. These biplots can reveal overall patterns of expression as well as potential problematic samples prior to downstream analysis. For simple analyses we expect to see the ‘main’ effect primarily along the first component.\nI like to use the PCAtools package for quickly computing and plotting principal components. For more complicated experiments I have also found UMAP (see coriell::UMAP()) to be useful for dimensionality reduction (although using UMAP is not without its problems for biologists).\n\nlibrary(PCAtools)\n\n\n# Perform PCA on the 20% most variable genes\n# Center and scale the variable after selecting most variable\npca_result &lt;- pca(\n  logcounts, \n  metadata = y$samples, \n  center = TRUE, \n  scale = TRUE, \n  removeVar = 0.8\n  )\n\n# Show the PCA biplot\nbiplot(\n  pca_result, \n  colby = \"group\", \n  hline = 0, \n  vline = 0, \n  hlineType = 2, \n  vlineType = 2, \n  legendPosition = \"bottom\",\n  title = \"PCA\",\n  caption = \"20% Most Variable Features\"\n  )\n\n\n\n\n\n\n\n\n\n\nAssessing global scaling normalization assumptions\nMost downstream differential expression testing methods apply a global scaling normalization factor to each library prior to DE testing. Applying these normalization factors when there are global expression differences can lead to spurious results. In typical experiments this is usually not a problem but when dealing with cancer or epigenetic drug treatment this can actually lead to many problems if not identified.\nTo identify potential violations of global scaling normalization I use the quantro R package. quantro uses two data driven approaches to assess the appropriateness of global scaling normalization. The first involves testing if the medians of the distributions differ between groups. These differences could indicate technical or real biological variation. The second test assesses the ratio of between group variability to within group variability using a permutation test similar to an ANOVA. If this value is large, it suggests global adjustment methods might not be appropriate.\n\nlibrary(quantro)\n\n\n# Initialize multiple (8) cores for permutation testing\ndoParallel::registerDoParallel(cores = 8)\n\n# Compute the qstat on the filtered libraries\nqtest &lt;- quantro(y$counts, groupFactor = y$samples$group, B = 500)\n\nNow we can assess the results. We can use anova() to test for differences in medians across groups. Here, they do not significantly differ.\n\nanova(qtest)\n\nAnalysis of Variance Table\n\nResponse: objectMedians\n            Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngroupFactor  1  1984.5  1984.5  0.3813 0.5596\nResiduals    6 31225.5  5204.3               \n\n\nWe can also plot the results of the permutation test to see the between:within group ratios. Again, there are no large differences in this dataset suggesting that global scaling normalization such as TMM is appropriate.\n\nquantroPlot(qtest)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#differential-expression-testing-with-edger",
    "href": "differential-expression-analysis.html#differential-expression-testing-with-edger",
    "title": "Differential Expression Analysis",
    "section": "Differential expression testing with edgeR",
    "text": "Differential expression testing with edgeR\nAfter removing lowly expressed features and checking the assumptions of normalization we can perform downstream differential expression testing with edgeR. The edgeR manual contains a detailed explanation of all steps involved in differential expression testing.\nIn short, we need to specify the experimental design, estimate normalization factors, fit the models, and perform DE testing.\n\nCreating the experimental design\nMaybe the most important step in DE analysis is properly constructing a design matrix. The details of design matrices are outside of the scope of this tutorial but a good overview can be found here. Generally, your samples will fall nicely into several well defined groups, facilitating the use of a design matrix without an intercept e.g. design ~ model.matrix(~0 + group, ...). This kind of design matrix makes it relatively simple to construct contrasts that describe exactly what pairs of groups you want to compare.\nSince this example experiment is simply comparing treatments to control samples we can model the differences in means by using a model with an intercept where the intercept is the mean of the control samples and the 2nd coefficient represents the differences in the treatment group.\n\n# Model with intercept\ndesign &lt;- model.matrix(~group, data = y$samples)\n\nWe can make an equivalent model and test without an intercept like so:\n\n# A means model\ndesign_no_intercept &lt;- model.matrix(~0 + group, data = y$samples)\n\n# Construct contrasts to test the difference in means between the groups\ncm &lt;- makeContrasts(\n  Treatment_vs_Control = grouptrt - groupuntrt,\n  levels = design_no_intercept\n)\n\nThe choice of which design is up to you. I typically use whatever is clearer for the experiment at hand. In this case, that is the model with an intercept.\n\n\nEstimating normalization factors\nWe use edgeR to calculate trimmed mean of the M-value (TMM) normalization factors for each library.\n\n# Estimate TMM normalization factors\ny &lt;- normLibSizes(y)\n\nWe can check the normalization by creating MA plots for each library. The bulk of the data should be centered on zero without any obvious differences in the logFC as a function of average abundance.\n\npar(mfrow = c(2, 4))\nfor (i in 1:ncol(y)) {\n  plotMD(cpm(y, log = TRUE), column = i)\n  abline(h = 0, lty = 2, col = \"red2\")\n}\n\n\n\n\n\n\n\n\n\nWhat to do if global scaling normalization is violated?\nAbove I described testing for violations of global scaling normalization. So what should we do if these assumptions are violated and we don’t have a good set of control genes or spike-ins etc.?\nIf we believe that the differences we are observing are due to true biological phenomena (this is a big assumption) then we can try to apply a method such as smooth quantile normalization to the data using the qsmooth package.\nBelow I will show how to apply qsmooth to our filtered counts and then calculate offsets to be used in downstream DE analysis with edgeR. Please note this is not a benchmarked or ‘official’ workflow just a method that I have implemented based on reading forums and github issues.\n\nlibrary(qsmooth)\n\n\n# Compute the smooth quantile factors \nqs &lt;- qsmooth(y$counts, group_factor = y$samples$group)\n\n# Extract the qsmooth transformed data\nqsd &lt;- qsmoothData(qs)\n\n# Calculate offsets to be used by edgeR in place of norm.factors\n# Offsets are on the natural log scale. Add a small offset to avoid\n# taking logs of zero \noffset &lt;- log(y$counts + 0.1) - log(qsd + 0.1)\n\n# Scale the offsets for internal usage by the DGEList object\n# Now the object is ready for downstream analysis\ny &lt;- scaleOffset(y, offset = offset)\n\n# To create logCPM values with the new norm factors use\nlcpm &lt;- cpm(y, offset = y$offset, log = TRUE)\n\n\n\n\nFit the model\nNew in edgeR 4.0 is the ability to estimate dispersions while performing the model fitting step. I typically tend to ‘robustify’ the fit to outliers. Below I will perform dispersion estimation in legacy mode so that we can use competitive gene set testing later. If we want to use the new workflow we can use the following:\n\n# edgeR 4.0 workflow\nfit &lt;- glmQLFit(y, design, legacy = FALSE, robust = TRUE)\n\nWe will continue with the legacy workflow.\n\ny &lt;- estimateDisp(y, design, robust = TRUE)\nfit &lt;- glmQLFit(y, design, robust = TRUE, legacy = TRUE)\n\nIt’s always a good idea at this step to check some of the diagnostic plots from edgeR\n\n# Show the biological coefficient of variation\nplotBCV(y)\n\n\n\n\n\n\n\n# Show the dispersion estimates\nplotQLDisp(fit)\n\n\n\n\n\n\n\n\n\n\nTest for differential expression\nNow that the models have been fit we can test for differential expression.\n\n# Test the treatment vs control condition\nqlf &lt;- glmQLFTest(fit, coef = 2)\n\nOften it is more biologically relevant to give more weight to higher fold changes. This can be achieved using glmTreat(). NOTE do not use glmQLFTest() and then filter by fold-change - you destroy the FDR correction!\nWhen testing against a fold-change we can use relatively modest values since the fold-change must exceed this threshold before being considered for significance. Values such as log2(1.2) or log2(1.5) work well in practice.\n\ntrt_vs_control_fc &lt;- glmTreat(fit, coef = 2, lfc = log2(1.2))\n\nIn any case, the results of the differential expression test can be extracted to a data.frame for downstream plotting with coriell::edger_to_df(). This function simply returns a data.frame of all results from the differential expression object in the same order as y. (i.e. topTags(..., n=Inf, sort.by=\"none\"))\n\nde_result &lt;- edger_to_df(qlf)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#plotting-de-results",
    "href": "differential-expression-analysis.html#plotting-de-results",
    "title": "Differential Expression Analysis",
    "section": "Plotting DE results",
    "text": "Plotting DE results\nThe two most common plots for differential expression analysis results are the volcano plot and the MA plot. Volcano plots display the negative log10 of the significance value on the y-axis vs the log2 fold-change on the x-axis. MA plots show the average expression of the gene on the x-axis vs the log2 fold-change of the gene on the y-axis. The coriell package includes functions for producing both.\n\nlibrary(patchwork)\n\n\n# Create a volcano plot of the results\nv &lt;- plot_volcano(de_result, fdr = 0.05) \n\n# Create and MA plot of the results\nm &lt;- plot_md(de_result, fdr = 0.05) \n\n# Patch both plots together\n(v | m) + \n  plot_annotation(title = \"Treatment vs. Control\") &\n  theme_coriell()\n\n\n\n\n\n\n\n\nThe coriell package also has a function for quickly producing heatmaps with nice defaults for RNA-seq. Sometimes it’s useful to show the heatmaps of the DE genes.\n\n# Compute logCPM values after normalization\nlcpm &lt;- cpm(y, log = TRUE)\n\n# Determine which of the genes in the result were differentially expressed\nis_de &lt;- de_result$FDR &lt; 0.05\n\n# Produce a heatmap from the DE genes\nquickmap(\n  x = lcpm[is_de, ], \n  metadata = y$samples[, \"group\", drop = FALSE],\n  main = \"Differentially Expressed Genes\"\n  )",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#competitive-gene-set-testing-with-camera",
    "href": "differential-expression-analysis.html#competitive-gene-set-testing-with-camera",
    "title": "Differential Expression Analysis",
    "section": "Competitive gene set testing with camera()",
    "text": "Competitive gene set testing with camera()\nI’ve recently become aware of some of the problems with gene set enrichment analysis using the fgsea package. Following Gordon Smyth’s advice, I have switched all of my pipelines to using competitive gene set testing (when appropriate) in limma to avoid problems with correlated genes.\nBelow we use the msigdbr R package to retrieve HALLMARK gene sets and then use edgeR::camera() for gene set testing.\n\nlibrary(msigdbr)\n\n\n# Get the HALLMARK gene set data\nmsigdb_hs &lt;- msigdbr(species = \"Homo sapiens\", category = \"H\")\n\nWarning: The `category` argument of `msigdbr()` is deprecated as of msigdbr 10.0.0.\nℹ Please use the `collection` argument instead.\n\n# Split into list of gene names per HALLMARK pathway\nmsigdb_hs &lt;- split(as.character(msigdb_hs$gene_symbol), msigdb_hs$gs_name)\n\n# Convert the gene sets into lists of indeces for edgeR\nidx &lt;- ids2indices(gene.sets = msigdb_hs, identifiers = y$genes$gene_name)\n\nPerform gene set testing. Note here we can use camera() mroast(), or romer() depending on the hypothesis being tested. The above setup code provides valid input for all of the above functions.\nSee this comment from Aaron Lun describing the difference between camera() and roast(). For GSEA like hypothesis we can use romer()\n\nroast() performs a self-contained gene set test, where it looks for any DE within the set of genes. camera() performs a competitive gene set test, where it compares the DE within the gene set to the DE outside of the gene set.\n\n\n# Use camera to perform competitive gene set testing\ncamera_result &lt;- camera(y, idx, design, contrast = 2)\n\n# Use mroast for rotational gene set testing - bump up number of rotations\nmroast_result &lt;- mroast(y, idx, design, contrast = 2, nrot = 1e4)\n\n# Use romer for GSEA like hypothesis testing\nromer_result &lt;- romer(y, idx, design, contrast = 2)\n\nWe can also perform a pre-ranked version of the camera test using cameraPR(). To use the pre-ranked version we need to create a ranking statistic. The suggestion from Gordon Smyth is to derive a z-statistic from the F-scores like so:\n\nt_stat &lt;- sign(de_result$logFC) * sqrt(de_result$`F`)\nz &lt;- zscoreT(t_stat, df = qlf$df.total)\n\n# Name the stat vector with the gene names \nnames(z) &lt;- de_result$gene_name\n\n# Use the z-scores as the ranking stat for cameraPR\ncamera_pr_result &lt;- cameraPR(z, idx)\n\nAnother useful plot to show following gene set testing is a barcodeplot. We The barcodeplot displays the enrichment of a given signature for a ranked list of genes. The limma::barcodeplot() function allows us to easily create these plots for any of the gene sets of interest using any ranking stat of our choice.\n\n# Show barcodeplot using the z-scores\nbarcodeplot(\n  z, \n  index = idx[[\"HALLMARK_ANDROGEN_RESPONSE\"]], \n  main = \"HALLMARK_ANDROGEN_RESPONSE\",\n  xlab = \"z-score\"\n  )\n\n\n\n\n\n\n\n# Or you can use the logFC\nbarcodeplot(\n  de_result$logFC, \n  index = idx[[\"HALLMARK_ANDROGEN_RESPONSE\"]], \n  main = \"HALLMARK_ANDROGEN_RESPONSE\",\n  xlab = \"logFC\"\n  )",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#gene-ontology-go-over-representation-test",
    "href": "differential-expression-analysis.html#gene-ontology-go-over-representation-test",
    "title": "Differential Expression Analysis",
    "section": "Gene ontology (GO) over-representation test",
    "text": "Gene ontology (GO) over-representation test\nOver-representation analysis can be performed with the clusterProfiler package. Here, instead of using the entire gene list as input we select separate sets of up and down-regulated genes and test to see if these sets are enriched in our differentially expressed gene list.\n\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db)\n\n\n# Split the genes into up and down\nup_genes &lt;- subset(\n  de_result, \n  FDR &lt; 0.05 & logFC &gt; 0, \n  \"gene_name\", \n  drop = TRUE\n  )\n\ndown_genes &lt;- subset(\n  de_result, \n  FDR &lt; 0.05 & logFC &lt; 0, \n  \"gene_name\", \n  drop = TRUE\n  )\n\n# Extract the list of all genes expressed in the experiment\n# to use as a background set\nuniverse &lt;- unique(y$genes$gene_name)\n\nCreate results objects for each set of genes\n\nego_up &lt;- enrichGO(\n  gene = up_genes,\n  universe = universe,\n  OrgDb = org.Hs.eg.db,\n  keyType = \"SYMBOL\",\n  ont = \"ALL\",\n  pAdjustMethod = \"BH\",\n  pvalueCutoff = 0.01,\n  qvalueCutoff = 0.05,\n  readable = TRUE\n  )\n\nego_down &lt;- enrichGO(\n  gene = down_genes,\n  universe = universe,\n  OrgDb = org.Hs.eg.db,\n  keyType = \"SYMBOL\",\n  ont = \"ALL\",\n  pAdjustMethod = \"BH\",\n  pvalueCutoff = 0.01,\n  qvalueCutoff = 0.05,\n  readable = TRUE\n  )\n\nThese results can be converted to data.frames and combined with:\n\nego_up_df &lt;- data.frame(ego_up)\nego_down_df &lt;- data.frame(ego_down)\n\nego_df &lt;- data.table::rbindlist(\n  list(up = ego_up_df, down = ego_down_df), \n  idcol = \"Direction\"\n  )\n\nOr the results can be plotted as dotplots with:\n\nd1 &lt;- dotplot(ego_up) + labs(title = \"Up-regulated genes\")\nd2 &lt;- dotplot(ego_down) + labs(title = \"Down-regulated genes\")\n\nd1 | d2\n\n\n\n\n\n\n\n\nYou can also create a nice enrichment map showing similarity between the significant GO terms like so:\n\nem_up &lt;- enrichplot::pairwise_termsim(ego_up)\nem_down &lt;- enrichplot::pairwise_termsim(ego_down)\n\np1 &lt;- enrichplot::emapplot(em_up, showCategory = 10, min_edge = 0.5) +\n  labs(title = \"Up-regulated genes\")\np2 &lt;- enrichplot::emapplot(em_down, showCategory = 10, min_edge = 0.5) +\n  labs(title = \"Down-regulated genes\")\n\np1 | p2",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#session-info",
    "href": "differential-expression-analysis.html#session-info",
    "title": "Differential Expression Analysis",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Pop!_OS 22.04 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] org.Hs.eg.db_3.21.0         AnnotationDbi_1.70.0       \n [3] clusterProfiler_4.16.0      msigdbr_25.1.1             \n [5] patchwork_1.3.1             quantro_1.40.0             \n [7] PCAtools_2.20.0             ggrepel_0.9.6              \n [9] coriell_0.17.0              ggplot2_3.5.2              \n[11] edgeR_4.6.3                 limma_3.64.1               \n[13] airway_1.28.0               SummarizedExperiment_1.38.1\n[15] Biobase_2.68.0              GenomicRanges_1.60.0       \n[17] GenomeInfoDb_1.44.1         IRanges_2.42.0             \n[19] S4Vectors_0.46.0            BiocGenerics_0.54.0        \n[21] generics_0.1.4              MatrixGenerics_1.20.0      \n[23] matrixStats_1.5.0          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.5.1             BiocIO_1.18.0            \n  [3] ggplotify_0.1.2           bitops_1.0-9             \n  [5] R.oo_1.27.1               tibble_3.3.0             \n  [7] preprocessCore_1.70.0     XML_3.99-0.18            \n  [9] lifecycle_1.0.4           doParallel_1.0.17        \n [11] lattice_0.22-7            MASS_7.3-65              \n [13] base64_2.0.2              scrime_1.3.5             \n [15] magrittr_2.0.3            minfi_1.54.1             \n [17] rmarkdown_2.29            yaml_2.3.10              \n [19] ggtangle_0.0.7            doRNG_1.8.6.2            \n [21] askpass_1.2.1             cowplot_1.2.0            \n [23] DBI_1.2.3                 RColorBrewer_1.1-3       \n [25] abind_1.4-8               quadprog_1.5-8           \n [27] R.utils_2.13.0            purrr_1.1.0              \n [29] RCurl_1.98-1.17           yulab.utils_0.2.0        \n [31] GenomeInfoDbData_1.2.14   enrichplot_1.28.2        \n [33] irlba_2.3.5.1             tidytree_0.4.6           \n [35] rentrez_1.2.4             genefilter_1.90.0        \n [37] pheatmap_1.0.13           annotate_1.86.1          \n [39] dqrng_0.4.1               DelayedMatrixStats_1.30.0\n [41] codetools_0.2-20          DelayedArray_0.34.1      \n [43] DOSE_4.2.0                xml2_1.3.8               \n [45] tidyselect_1.2.1          aplot_0.2.8              \n [47] UCSC.utils_1.4.0          farver_2.1.2             \n [49] ScaledMatrix_1.16.0       beanplot_1.3.1           \n [51] illuminaio_0.50.0         GenomicAlignments_1.44.0 \n [53] jsonlite_2.0.0            multtest_2.64.0          \n [55] survival_3.8-3            iterators_1.0.14         \n [57] foreach_1.5.2             tools_4.5.1              \n [59] treeio_1.32.0             Rcpp_1.1.0               \n [61] glue_1.8.0                SparseArray_1.8.0        \n [63] xfun_0.52                 qvalue_2.40.0            \n [65] dplyr_1.1.4               HDF5Array_1.36.0         \n [67] withr_3.0.2               fastmap_1.2.0            \n [69] rhdf5filters_1.20.0       openssl_2.3.3            \n [71] digest_0.6.37             rsvd_1.0.5               \n [73] gridGraphics_0.5-1        R6_2.6.1                 \n [75] colorspace_2.1-1          GO.db_3.21.0             \n [77] RSQLite_2.4.2             R.methodsS3_1.8.2        \n [79] h5mread_1.0.1             tidyr_1.3.1              \n [81] data.table_1.17.8         rtracklayer_1.68.0       \n [83] httr_1.4.7                htmlwidgets_1.6.4        \n [85] S4Arrays_1.8.1            pkgconfig_2.0.3          \n [87] gtable_0.3.6              rdist_0.0.5              \n [89] blob_1.2.4                siggenes_1.82.0          \n [91] XVector_0.48.0            htmltools_0.5.8.1        \n [93] fgsea_1.34.2              scales_1.4.0             \n [95] png_0.1-8                 ggfun_0.2.0              \n [97] knitr_1.50                rstudioapi_0.17.1        \n [99] tzdb_0.5.0                reshape2_1.4.4           \n[101] rjson_0.2.23              nlme_3.1-168             \n[103] curl_6.4.0                bumphunter_1.50.0        \n[105] cachem_1.1.0              rhdf5_2.52.1             \n[107] stringr_1.5.1             KernSmooth_2.23-26       \n[109] parallel_4.5.1            restfulr_0.0.16          \n[111] GEOquery_2.76.0           pillar_1.11.0            \n[113] grid_4.5.1                reshape_0.8.10           \n[115] vctrs_0.6.5               BiocSingular_1.24.0      \n[117] beachmat_2.24.0           xtable_1.8-4             \n[119] evaluate_1.0.4            readr_2.1.5              \n[121] GenomicFeatures_1.60.0    cli_3.6.5                \n[123] locfit_1.5-9.12           compiler_4.5.1           \n[125] Rsamtools_2.24.0          rlang_1.1.6              \n[127] crayon_1.5.3              rngtools_1.5.2           \n[129] labeling_0.4.3            nor1mix_1.3-3            \n[131] mclust_6.1.1              fs_1.6.6                 \n[133] plyr_1.8.9                stringi_1.8.7            \n[135] viridisLite_0.4.2         BiocParallel_1.42.1      \n[137] assertthat_0.2.1          babelgene_22.9           \n[139] Biostrings_2.76.0         lazyeval_0.2.2           \n[141] bspm_0.5.7                GOSemSim_2.34.0          \n[143] Matrix_1.7-3              hms_1.1.3                \n[145] sparseMatrixStats_1.20.0  bit64_4.6.0-1            \n[147] Rhdf5lib_1.30.0           KEGGREST_1.48.1          \n[149] statmod_1.5.0             igraph_2.1.4             \n[151] memoise_2.0.1             ggtree_3.16.2            \n[153] fastmatch_1.1-6           bit_4.6.0                \n[155] gson_0.1.0                ape_5.8-1",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html",
    "href": "differential-methylation-analysis.html",
    "title": "Differential Methylation Analysis",
    "section": "",
    "text": "Download the data\nWelcome! In this tutorial, we’ll walk through the common steps of differential methylation analysis.\nFirst we will download the data from “GSE86297”.\nThis study investigates the onset and progression of de novo methylation. Growing oocytes from pre-pubertal mouse ovaries (post-natal days 7-18) isolated and sorted into the following, non-overlapping size categories: 40-45, 50-55 and 60-65μm with two biological replicates in each.\nThe metadata of the samples is described as:\nDownload the metadata from github",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#download-the-data",
    "href": "differential-methylation-analysis.html#download-the-data",
    "title": "Differential Methylation Analysis",
    "section": "",
    "text": "#!/usr/bin/env bash\n#\n# Download the data from GEO\n#\n# ----------------------------------------------------------------------------\n\nwget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE86nnn/GSE86297/suppl/GSE86297_RAW.tar\n\ntar -xvf GSE86297_RAW.tar\n\nGEO Source Group File\nGSM2299710 40-45um-A 40um GSM2299710_RRBS_40-45oocyte_LibA.cov.txt.gz\nGSM2299711 40-45um-B 40um GSM2299711_RRBS_40-45oocyte_LibB.cov.txt.gz\nGSM2299712 50-55um-A 50um GSM2299712_RRBS_50-55oocyte_LibA.cov.txt.gz\nGSM2299713 50-55um-B 50um GSM2299713_RRBS_50-55oocyte_LibB.cov.txt.gz\nGSM2299714 60-65um-A 60um GSM2299714_RRBS_60-65oocyte_LibA.cov.txt.gz\nGSM2299715 60-65um-B 60um GSM2299715_RRBS_60-65oocyte_LibB.cov.txt.gz\n\n#!/usr/bin/env bash\n#\n# Download the metadata from github\n#\n# ----------------------------------------------------------------------------\n\nwget https://github.com/coriell-research/2025-coriell-summer-internship/raw/refs/heads/main/MethylationMetadata.csv",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#the-bismark-cov.txt.gz-output-format",
    "href": "differential-methylation-analysis.html#the-bismark-cov.txt.gz-output-format",
    "title": "Differential Methylation Analysis",
    "section": "The Bismark cov.txt.gz output format",
    "text": "The Bismark cov.txt.gz output format\nThe Bismark coverage files include the data for each sample representing methylation in the CpG context. These files will contain 6 columns describing the: chromosome, start position, end position, methylation proportion in percentage, number of methylated C’s, and the number of unmethylated C’s.\n\ns1 &lt;- read.delim(\n  file=\"GSM2299710_RRBS_40-45oocyte_LibA.cov.txt.gz\", \n  header=FALSE, \n  nrows=6\n)\n\ns1\n\n&gt; s1\nV1 V2 V3 V4 V5 V6\n1 6 3121266 3121266 0.00 0 17\n2 6 3121296 3121296 0.00 0 17\n3 6 3179319 3179319 1.28 1 77\n4 6 3180316 3180316 4.55 1 21\n5 6 3182928 3182928 4.33 22 486\n6 6 3182937 3182937 5.37 61 1074",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#reading-in-the-data",
    "href": "differential-methylation-analysis.html#reading-in-the-data",
    "title": "Differential Methylation Analysis",
    "section": "Reading in the data",
    "text": "Reading in the data\nTo read in the data, we can use edgeR’s helper function readBismark2DGE\n\nmd &lt;- read.table(\"/path/to/metadata/MethylationMetadata.csv\", sep = \",\", header = TRUE)\nrownames(md) &lt;- md$Source\nmd$Filepath &lt;- vapply(\n  md$File,\n  function(x) list.files(\n    path = \"/path/to/geo/data\",\n    pattern = x,\n    recursive = TRUE,\n    full.names = TRUE\n  ),\n  character(1)\n)\nyall &lt;- edgeR::readBismark2DGE(md$Filepath, sample.names = md$Source)\n\nThe edgeR package will format this data such that in yall$counts for each sample you will have two columns called “Me” which corresponds to the methylated reads and “Un” which corresponds to the unmethylated reads. Each row correpsonds to a CpG locus found in the files. The genomic coordinates of the CpGs are stored in yall$genes. yall$samples corresponds to the metadata of the samples. It might be helpful to add the metadata you download to the samples component.\n\nyall$samples &lt;- cbind(yall$samples, md[md[rep(seq_len(nrow(md)), each = 2),],])\nyall$samples$methylation &lt;- rep(c(\"Me\", \"Un\"), length(unique(yall$samples$Source)))",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#filtering-and-normalization",
    "href": "differential-methylation-analysis.html#filtering-and-normalization",
    "title": "Differential Methylation Analysis",
    "section": "Filtering and Normalization",
    "text": "Filtering and Normalization\nFor this example we will remove the mitochondrial (MT) CpGs since they are typically not of interest.\n\ntable(yall$genes$Chr)\nyall &lt;- yall[yall$genes$Chr!=\"MT\", ]\n\nIt might be useful to order your chromosomes in genomic order.\n\nChrNames &lt;- c(1:19,\"X\",\"Y\")\nyall$genes$Chr &lt;- factor(yall$genes$Chr, levels=ChrNames)\no &lt;- order(yall$genes$Chr, yall$genes$Locus)\nyall &lt;- yall[o,]\n\nYou should also annotate your CpGs with the nearest gene transcription start site.\n\nTSS &lt;- nearestTSS(yall$genes$Chr, yall$genes$Locus, species=\"Mm\")\nyall$genes$EntrezID &lt;- TSS$gene_id\nyall$genes$Symbol &lt;- TSS$symbol\nyall$genes$Strand &lt;- TSS$strand\nyall$genes$Distance &lt;- TSS$distance\nyall$genes$Width &lt;- TSS$width\n\nThe Distance column will describe the genomic distance in base pairs of the nearest gene to the CpG. If the distance is negative the TSS is upstream of the CpG and positive distances are downstream.\nBefore we can examine the methylation of the samples, we should remove any CpGs that have low coverage. In order to do this we should sum the Me and Un columns for each sample to obtain the total coverage for the loci.\n\nMe &lt;- yall$counts[,yall$samples$methylation == \"Me\"]\nUn &lt;- yall$counts[,yall$samples$methylation == \"Un\"]\n\nCoverage &lt;- Me + Un\n\nTo be conservative in our filtering we can make sure every sample has sufficient coverage for it to be included.\n\nhasCoverage &lt;- rowSums(Coverage &gt;= 8) == 6\n\nIf you have an adequate number of CpGs to test, you may proceed. If the number of CpGs seems to low, maybe consider other filtering strategies.\nWe will also filer out CpGs that are never methylated or are always methylated because they will provide little information in the differential methylation and will dilute our testing.\n\nhasBoth &lt;- rowSums(Me) &gt; 0 & rowSums(Un) &gt; 0\n\nYou should check the table to get an understanding of how many CpGs fell into each category.\n\ntable(hasCoverage, hasBoth)\n\nNow you can filter your DGEList for the desired CpGs to test in downstream analysis.\n\ny &lt;- yall[hasCoverage & hasBoth,,keep.lib.sizes=FALSE]\n\nWe need to calculate the actual library sizes since the reads are divides into the Un and Me categories.\n\nTotalLibSize &lt;- 0.5 * y$samples$lib.size[y$samples$methylation == \"Me\"] + \n                0.5 * y$samples$lib.size[y$samples$methylation == \"Un\"]\n\ny$samples$lib.size &lt;- rep(TotalLibSize, each = 2)",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#quality-control",
    "href": "differential-methylation-analysis.html#quality-control",
    "title": "Differential Methylation Analysis",
    "section": "Quality Control",
    "text": "Quality Control\nTo observe the methylation/coverage distributions for a given sample we can plot histograms of each sample.\nYou can look at the sample distribution for coverage to see if there might be any max coverage you wish to filter for. This might be indicated by a secondary peak at the high coverage ranges.\n\nMe &lt;- y$counts[, y$samples$methylation == \"Me\"]\nUn &lt;- y$counts[, y$samples$methylation == \"Un\"]\nCoverage &lt;- Me + Un\n\npar(mfrow = c(2, 3))\nfor (i in md$Source) {\n  hist(Me/Coverage, breaks = 100, main = i)\n}\n\npar(mfrow = c(2, 3))\nfor (i in md$Source) {\n  hist(log10(Coverage), breaks = 100, main = i)\n}\n\n\n\n\nHistogram of Methylation\n\n\n\n\n\nHistogram of Coverage\n\n\nTo observe sample concordance you can use the plot_cor_pairs function in the coriell package.\n\ncoriell::plot_cor_pairs(Me/Coverage, cex_labels = 1)",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#data-exploration",
    "href": "differential-methylation-analysis.html#data-exploration",
    "title": "Differential Methylation Analysis",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe data can be explored by generating PCA plots of the methylation level in the M-value format. More information of the M-value can be found “here”. An M-value is calculated by taking the log of the ratio of methylated and unmethylated C’s. This is equivalent to the difference between methylated to the difference between methylated and unmethylated C’s on the log-scale. A prior count of 2 is added to avoid taking the log of zero.\n\nMe &lt;- y$counts[, y$samples$methylation == \"Me\"]\nUn &lt;- y$counts[, y$samples$methylation == \"Un\"]\nM &lt;- log2(Me + 2) - log2(Un + 2)\ncolnames(M) &lt;- md$Source\n\nWe can now use these M-values to generate a PCA biplot using the PCAtools package.\n\npca &lt;- PCAtools::pca(\n  M,\n  metadata = md\n)\n\nPCAtools::biplot(\n  pca,\n  colby = \"Group\",\n  title = \"Principal Component Analysis of the M-values\"\n)\n\n\nIt might be of interest to observe the sample to sample distances as well.\n\ncoriell::plot_dist(M, metadata = md[, \"Group\", drop = FALSE])",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#design-matrix",
    "href": "differential-methylation-analysis.html#design-matrix",
    "title": "Differential Methylation Analysis",
    "section": "Design Matrix",
    "text": "Design Matrix\nOne aim of this study is to identify differentially methylated (DM) loci between the different cell populations. In edgeR, this can be done by fitting linear models under a specified design matrix and testing for corresponding coefficients or contrasts. A basic sample-level design matrix can be made as follows.\nWe then expand this to the full design which models sample and methylation effects.\nThe first six columns represent the sample coverage effects. The last three columns represent the methylation levels (in logit units) in the three groups.\n\ndesignSL &lt;- model.matrix(~0+Group, data=md)\ndesign &lt;- edgeR::modelMatrixMeth(designSL)",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "differential-methylation-analysis.html#differential-methylation-analysis-at-cpg-loci",
    "href": "differential-methylation-analysis.html#differential-methylation-analysis-at-cpg-loci",
    "title": "Differential Methylation Analysis",
    "section": "Differential methylation analysis at CpG loci",
    "text": "Differential methylation analysis at CpG loci\nFor simplicity, we only consider the CpG methylation in chromosome 1. We subset the coverage files so that they only contain methylation information of the first chromosome.\n\ny1 &lt;- y[y$genes$Chr==1,]\n\nThen we proceed to testing for differentially methylated CpG sites between different groups. We fit quasi NB GLM for all the CpG loci using the glmQLFit function.\n\nfit &lt;- glmQLFit(y1, design)\n\nWe identify differentially methylated CpG loci between the 40-45 and 60-65μm group using the likelihood-ratio test. The contrast corresponding to this comparison is constructed using the makeContrasts function.\n\ncontr &lt;- makeContrasts(Group60vs40 = Group60um - Group40um, levels=design)\nqlf &lt;- glmQLFTest(fit, contrast=contr)\n\nThe contrast object is a matrix showing the contrast desired\n&gt; contr\n           Contrasts\nLevels      Group60vs40\n  Sample1             0\n  Sample2             0\n  Sample3             0\n  Sample4             0\n  Sample5             0\n  Sample6             0\n  Group40um          -1\n  Group50um           0\n  Group60um           1\nWe could also make multiple contrasts in one go and then select the contrast we wish to use like this.\n\ncontr &lt;- makeContrasts(\n  Group60vs40 = Group60um - Group40um,\n  Group50vs40 = Group50um - Group40um, \n  levels=design\n)\nqlf &lt;- glmQLFTest(fit, contrast=contr[,\"Group60vs40\"])\n\n&gt; contr\n           Contrasts\nLevels      Group60vs40 Group50vs40\n  Sample1             0           0\n  Sample2             0           0\n  Sample3             0           0\n  Sample4             0           0\n  Sample5             0           0\n  Sample6             0           0\n  Group40um          -1          -1\n  Group50um           0           1\n  Group60um           1           0\nThe top set of most significant DMRs can be examined with topTags. Here, positive log-fold changes represent CpG sites that have higher methylation level in the 60-65μm group com- pared to the 40-45μm group. Multiplicity correction is performed by applying the Benjamini- Hochberg method on the p-values, to control the false discovery rate (FDR).\n\ntopTags(qlf)\n\n&gt; topTags(qlf)\nCoefficient:  -1*Group40um 1*Group60um \n            Chr     Locus     logFC   logCPM        F       PValue          FDR\n1-131987595   1 131987595 10.741404 2.698587 31.70873 1.850730e-08 0.0001409481\n1-120170060   1 120170060  7.855076 4.531948 31.23248 2.362954e-08 0.0001409481\n1-183357406   1 183357406  8.399456 4.272956 30.78670 2.970455e-08 0.0001409481\n1-172206570   1 172206570 10.110386 2.706416 29.93463 4.601316e-08 0.0001637493\n1-172206751   1 172206751 13.918907 1.294445 28.71671 8.607148e-08 0.0002302044\n1-120169950   1 120169950  9.231213 3.469975 28.21772 1.112752e-07 0.0002302044\n1-169954561   1 169954561 12.217222 2.352360 28.08869 1.189199e-07 0.0002302044\n1-141992739   1 141992739 11.276043 1.335515 27.73221 1.428846e-07 0.0002302044\n1-92943747    1  92943747  9.580566 2.358125 27.58701 1.539852e-07 0.0002302044\n1-36500213    1  36500213  9.957566 2.195456 27.49192 1.617172e-07 0.0002302044\nThe total number of DMRs in each direction at a FDR of 5% can be examined with decideTests.\n\nsummary(decideTests(qlf))\n\n&gt; summary(decideTests(qlf))\n       -1*Group40um 1*Group60um\nDown                          0\nNotSig                    13252\nUp                          983\nThe differential methylation results can be visualized using an MD plot. The difference of the M-value for each CpG site is plotted against the average abundance of that CpG site. Significantly DMRs at a FDR of 5% are highlighted.\n\nplotMD(qlf)\n\n\nWe can also plot volcanoes by calculating the difference in methylation of each group since log fold changes of these test might be hard to interpret as far as methylation levels go.\n\nMe &lt;- y1$counts[,y1$samples$methylation == \"Me\"]\nUn &lt;- y1$counts[,y1$samples$methylation == \"Un\"]\nCoverage &lt;- Me + Un\nBeta &lt;- Me/Coverage\ncolnames(Beta) &lt;- rownames(md)\n\ncontr &lt;- makeContrasts(\n  Group60vs40 = Group60um - Group40um,\n  levels=designSL\n)\ncontr &lt;- limma::contrastAsCoef(designSL, contr)\ngroup1Methylation &lt;- rowMeans(Beta[,which(contr$design[,\"Group60vs40\"] &gt; 0)])\ngroup2Methylation &lt;- rowMeans(Beta[,which(contr$design[,\"Group60vs40\"] &lt; 0)])\ndiffMethylation &lt;- group1Methylation - group2Methylation\n\ndataframe &lt;- coriell::edger_to_df(qlf)\ndataframe$group1 &lt;- group1Methylation\ndataframe$group2 &lt;- group2Methylation\ndataframe$diffmeth &lt;- diffMethylation\n\nUsing these values we can plot a volcano plot using the coriell package.\n\ncoriell::plot_volcano(dataframe, x = \"diffmeth\", y = \"FDR\", lfc = 0, fdr = 0.05) +\n  coord_cartesian(xlim = c(-1,1))",
    "crumbs": [
      "Differential Methylation Analysis"
    ]
  },
  {
    "objectID": "atacseq-analysis.html",
    "href": "atacseq-analysis.html",
    "title": "ATAC-seq Analysis",
    "section": "",
    "text": "Filter raw fastq files\nWork in progress\nUse fastp to perform quality trimming on raw fastq files. Turn on adapter detection for paired end reads.\n#!/usr/bin/env bash\n#\n# Run fastp on the raw fastq files\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nFQ=/path/to/raw-fastq/00_fastq\nSAMPLES=/path/to/sample-names.txt   \nOUT=/path/to/01_fastp\nTHREADS=12\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    fastp -i $FQ/${SAMPLE}_R1.fastq.gz \\\n          -I $FQ/${SAMPLE}_R2.fastq.gz \\\n          -o $OUT/${SAMPLE}.trimmed.1.fq.gz \\\n          -O $OUT/${SAMPLE}.trimmed.2.fq.gz \\\n          -h $OUT/${SAMPLE}.fastp.html \\\n          -j $OUT/${SAMPLE}.fastp.json \\\n            --detect_adapter_for_pe \\\n          -w $THREADS\ndone\nsample-names.txt is a plain text file listing the basenames for all samples. For example, if you have “sample1_R1.fq.gz”, “sample2_R1.fq.gz”, “sample3_R1.fq.gz” then sample-names.txt would be:\nThis file gets used throughout.",
    "crumbs": [
      "ATAC-seq Analysis"
    ]
  },
  {
    "objectID": "atacseq-analysis.html#filter-raw-fastq-files",
    "href": "atacseq-analysis.html#filter-raw-fastq-files",
    "title": "ATAC-seq Analysis",
    "section": "Filter raw fastq files",
    "text": "Filter raw fastq files\nUse fastp to perform quality trimming on raw fastq files. Turn on adapter detection for paired end reads.\n\n#!/usr/bin/env bash\n#\n# Run fastp on the raw fastq files\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nFQ=/path/to/raw-fastq/00_fastq\nSAMPLES=/path/to/sample-names.txt   \nOUT=/path/to/01_fastp\nTHREADS=12\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    fastp -i $FQ/${SAMPLE}_R1.fastq.gz \\\n          -I $FQ/${SAMPLE}_R2.fastq.gz \\\n          -o $OUT/${SAMPLE}.trimmed.1.fq.gz \\\n          -O $OUT/${SAMPLE}.trimmed.2.fq.gz \\\n          -h $OUT/${SAMPLE}.fastp.html \\\n          -j $OUT/${SAMPLE}.fastp.json \\\n            --detect_adapter_for_pe \\\n          -w $THREADS\ndone\n\nsample-names.txt is a plain text file listing the basenames for all samples. For example, if you have “sample1_R1.fq.gz”, “sample2_R1.fq.gz”, “sample3_R1.fq.gz” then sample-names.txt would be:\nsample1\nsample2\nsample3\nThis file gets used throughout."
  },
  {
    "objectID": "atacseq-analysis.html#perform-alignment-with-bowtie2",
    "href": "atacseq-analysis.html#perform-alignment-with-bowtie2",
    "title": "ATAC-seq Analysis",
    "section": "Perform alignment with Bowtie2",
    "text": "Perform alignment with Bowtie2\nUse --very-sensitive mode and set max insertion size to 1,000. The output is piped to samtools fixmate for adding mate tag information and then to samtools sort -n to create name sorted BAM files.\nIt’s assumed that you have a bowtie2 index generated for your species of interest. The number of jobs, threads, and memory allowed per thread will all be machine dependent.\n\n#!/usr/bin/env bash\n#\n# Align trimmed reads with bowtie2, fixmate tags, and output name sorted BAMs\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nFQ=/path/to/01_fastp\nSAMPLES=/path/to/sample-names.txt   \nOUT=/path/to/02_align\nIDX=/path/to/bt2_idx\nINSERT=1000\nJOBS=6\nTHREADS=4\n\nmkdir -p $OUT\n\nparallel --jobs $JOBS \\\n  \"bowtie2 -x $IDX \\\n    -1 $FQ/{}.trimmed.1.fq.gz \\\n    -2 $FQ/{}.trimmed.2.fq.gz \\\n    --very-sensitive \\\n    --threads $THREADS \\\n    --maxins $INSERT | \\\n    samtools fixmate -m -@$THREADS - - | \\\n    samtools sort -n -@$THREADS -m4G -o $OUT/{}.bam - \" :::: $SAMPLES"
  },
  {
    "objectID": "atacseq-analysis.html#call-peaks-with-genrich",
    "href": "atacseq-analysis.html#call-peaks-with-genrich",
    "title": "ATAC-seq Analysis",
    "section": "Call peaks with Genrich",
    "text": "Call peaks with Genrich\nGenrich is a fast peak caller designed to work with single samples or replicates. It has a dedicated option for peak calling ATAC-seq samples.\nThe options used below:\n\nOutput pileups over called regions in a pseudo bedfile (-k)\nOutput the log file (-f) which can be used to recall peaks with different -p and -a values\nExclude regions in blacklist (-E)\nExclude reads mapping to chrM (-e)\nCall in ATAC-seq mode (-j)\nRemove PCR duplicates (-r)\nSpecify p-value threshold for peak calling (-p)\nSkip name sorting check (-S)\n\n\n#!/usr/bin/env bash\n#\n# Call ATAC-seq peaks using Genrich on the name sorted BAMs\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nSAMPLES=/path/to/sample-names.txt   \nBAM=/path/to/02_align\nOUT=/path/to/03_callpeak\nEXCLUDE=/path/to/excluderanges.bed\nPVAL=0.05\nJOBS=6\n\nparallel --jobs $JOBS \\\n    \"Genrich -t $BAM/{}.bam \\\n        -o $OUT/{}.narrowPeak \\\n        -k $OUT/{}.pileup.txt \\\n        -f $OUT/{}.log \\\n        -E $EXCLUDE \\\n        -p $PVAL -j -r -e chrM -S \" :::: $SAMPLES\n\n\nExclusion lists\nThe “excluderanges.bed” file used above contains blacklisted regions of high-mappability that can be excluded from peak calling. This BED file can be created using the excluderanges R package. For example, to generate the excluded regions for mm39:\n\nsuppressMessages(library(GenomicRanges))\nsuppressMessages(library(AnnotationHub))\n\nah <- AnnotationHub()\nquery_data <- subset(ah, preparerclass == \"excluderanges\")\nmm39_exclude_gr <- query_data[[\"AH107321\"]]\nrtracklayer::export.bed(mm39_exclude_gr, \"mm39-excluderanges.bed\")"
  },
  {
    "objectID": "atacseq-analysis.html#determine-consensus-peaks-for-replicate-samples",
    "href": "atacseq-analysis.html#determine-consensus-peaks-for-replicate-samples",
    "title": "ATAC-seq Analysis",
    "section": "Determine consensus peaks for replicate samples",
    "text": "Determine consensus peaks for replicate samples\nAfter peak calling finishes, I create consensus peak calls for replicate samples. The consensus peak calls can be used in downstream differential accessibility analysis or for visualization. Creating consensus peak calls can be done in R by requiring all or some peaks to be called across replicate samples.\n\nsuppressPackageStartupMessages(library(here))\nsuppressPackageStartupMessages(library(GenomicRanges))\n\n\n# Read in peak calls\npeak_files <- list.files(here(\"data\", \"04_callpeak\"), pattern=\"*.narrowPeak\", full.names=TRUE)\nnames(peak_files) <- gsub(\".narrowPeak\", \"\", basename(peak_files))\npeak_calls <- lapply(peak_files, rtracklayer::import)\n\n# Separate into groups \ncontrol_calls <- peak_calls[c(\"control1\", \"control2\", \"control3\")]\ntreatment_calls <- peak_calls[c(\"treatment1\", \"treatment2\", \"treatment3\")]\ncontrol_calls <- GRangesList(control_calls)\ntreatment_calls <- GRangesList(treatment_calls)\n\n# Compute coverage across ranges\ncontrol_coverage <- coverage(control_calls)\ntreatment_coverage <- coverage(treatment_calls)\n\n# Determine regions with coverage in N replicates -- here require all 3 to have the same peaks\ncontrol_covered <- GRanges(slice(control_coverage, lower=length(control_calls), rangesOnly=TRUE))\ntreatment_covered <- GRanges(slice(treatment_coverage, lower=length(treatment_calls), rangesOnly=TRUE))\n\n# Close gaps in the resulting regions -- min.gapwidth can be adjusted \ncontrol_consensus <- reduce(control_covered, min.gapwidth=101)\ntreatment_consensus <- reduce(treatment_covered, min.gapwidth=101)\nall_consensus <- union(control_consensus, treatment_consensus)\n\n# Export as BED files\nrtracklayer::export.bed(control_consensus, here(\"data\", \"04_callpeak\", \"control-consensus.bed\"))\nrtracklayer::export.bed(treatment_consensus, here(\"data\", \"04_callpeak\", \"treatment-consensus.bed\"))\nrtracklayer::export.bed(all_consensus, here(\"data\", \"04_callpeak\", \"consensus.bed\"))"
  },
  {
    "objectID": "atacseq-analysis.html#create-raw-signal-tracks",
    "href": "atacseq-analysis.html#create-raw-signal-tracks",
    "title": "ATAC-seq Analysis",
    "section": "Create raw signal tracks",
    "text": "Create raw signal tracks\nGenrich outputs a bedgraph-ish file using the -k flag that can be used to quickly create a a bigwig file of the raw signal which can be viewed in IGV. I find it useful to look at the raw signal tracks after peak calling so I can assess the absolute magnitude of the pileup that went into calling a peak. Then, if the peaks look too permissive or too stringent, adjust the peak calls.\nCreating bigwigs from the bedgraph-ish files can be accomplished using awk and bedGraphToBigWig from UCSC.\n\n#!/usr/bin/env bash\n#\n# Convert the pileups computed by Genrich from bedGraph-ish files to bigwigs\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nSAMPLES=/path/to/sample-names.txt   \nPEAKS=/path/to/03_callpeak\nOUT=/path/to/04_bg2bw\nCHROM_SIZES=/path/to/chrom.sizes\nJOBS=6\n\nmkdir -p $OUT\n\necho \"Extracting bedGraphs from experimental signal...\"\nparallel --jobs $JOBS \\\n  \"awk 'NR > 2 { print \\$1, \\$2, \\$3, \\$4 }' $PEAKS/{}.pileup.txt | \\\n  LC_ALL=C sort -k1,1 -k2,2n > $OUT/{}.sorted.bedGraph\" :::: $SAMPLES\n\necho \"Creating bigwigs from pileups...\"\nparallel --jobs $JOBS \"bedGraphToBigWig $OUT/{}.sorted.bedGraph $CHROM_SIZES $OUT/{}.bw\" :::: $SAMPLES\n\necho \"Cleaning up intermediate bedGraphs...\"\nfind $OUT -type f -name \"*.bedGraph\" -delete\n\n\nChrom sizes files\nYou can supply a URL to bedGraphToBigWig to get the chrom.sizes if you’re genome is supported. Otherwise, you can create a chrom.sizes file using another UCSC utility, faSize, and your genome fasta.\n\nfaSize -detailed -tab genome.fasta > genome.chrom.sizes"
  },
  {
    "objectID": "atacseq-analysis.html#atacseqqc",
    "href": "atacseq-analysis.html#atacseqqc",
    "title": "ATAC-seq Analysis",
    "section": "ATACSeqQC",
    "text": "ATACSeqQC\nTODO: complete this section\nThe ATACseqQC R package can be used to calculate various QC stats on the filtered BAM files. These should be assessed closely and compared to the ENCODE data standards."
  },
  {
    "objectID": "atacseq-analysis.html#differential-abundance-using-csaw-and-edger",
    "href": "atacseq-analysis.html#differential-abundance-using-csaw-and-edger",
    "title": "ATAC-seq Analysis",
    "section": "Differential abundance using csaw and edgeR",
    "text": "Differential abundance using csaw and edgeR\nTODO: elaborate on this section\ncsaw can be used to perform differential abundance over sliding windows or by counting reads in the consensus peaks defined above. This paper contains some code for performing and assessing ATAC-seq DA analysis using both methods.\nI usually generate read counts over the consensus peaks using featureCounts from the Rsubread package. An SAF formatted file can be generated easily from the “consensus.bed” file. To get the read counts to align with the Genrich called peaks, set the -read2Pos 5 flag to count alignments over the 5’ end of reads."
  },
  {
    "objectID": "atacseq-analysis.html#create-tss-and-region-plots-with-deeptools",
    "href": "atacseq-analysis.html#create-tss-and-region-plots-with-deeptools",
    "title": "ATAC-seq Analysis",
    "section": "Create TSS and region plots with deeptools",
    "text": "Create TSS and region plots with deeptools\ndeeptools can be used to compute normalized coverage files in bigwig format for visualization in IGV. Normalization in deeptools can be really context dependent and what normalization strategy to use can often be unclear.\nIf you’ve performed differential abundance analysis and have global scaling normalization factors, they can be used at this step to generate TMM normalized bigwigs. Below, scale each bigwig file using TMM scaling factors computed by edgeR::normLibSizes()\nFirst, compute the reciprocal of the TMM scaling factors in R:\n\n# This assumes you have used featureCounts and imported into edgeR DGEList object 'y'\ny <- normLibSizes(y)\n\n# Compute the scale factors for each library\nsf <- y$samples$norm.factors * y$samples$lib.size / 1e6   \n\n# Take the inverse of the scale factors for use in deeptools\ninv_sf <- 1 / sf\n\n# Write out to a csv file -- assuming the colnames match the file basenames\nfwrite(\n  data.frame(sample_name = colnames(y), \n             scale_factor=inv_sf), \n  \"scale-factors.csv\", \n  col.names=FALSE\n  )\n\nThen, to compute TMM scaled bigwigs using deeptools\n\n#!/usr/bin/env bash\n#\n# Compute TMM normalized coverage bigwigs for visualization\n#\n# 'deeptools' mamba env must be activated before running\n# -----------------------------------------------------------------------------\nSAMPLES=/path/to/sample-names.txt\nSCALE_FACTORS=/path/to/scale-factors.csv\nBLACKLIST=/path/to/excluderanges.bed\nCODING=/path/to/coding.bed\nPEAKS=/path/to/consensus.bed\nBAM=/path/to/02_align\nOUT=/path/to/05_deeptools\nJOBS=6\nTHREADS=4\n\nmkdir -p $OUT\n\necho \"Position sorting BAMs...\"\nparallel --jobs $JOBS \"samtools sort -@$THREADS -m4G -o $OUT/{}.sorted.bam $BAM/{}.bam\" :::: $SAMPLES\nparallel --jobs $JOBS \"samtools index $OUT/{}.sorted.bam\" :::: $SAMPLES\n\necho \"Computing coverage over all BAM files...\" \nparallel --csv --jobs $JOBS \\\n    \"bamCoverage --bam $BAM/{1}.sorted.bam \\\n                 --outFileName $OUT/{1}.bw \\\n                 --scaleFactor {2} \\\n                 --outFileFormat 'bigwig' \\\n                 --blackListFileName $BLACKLIST \\\n                 --numberOfProcessors $THREADS \\\n                 --ignoreDuplicates\" :::: $SCALE_FACTORS\n\necho \"Computing matrix over TSS regions...\"\ncomputeMatrix reference-point \\\n --regionsFileName  \\\n --scoreFileName $OUT/*.bw \\\n --outFileName $OUT/tss.mat.gz \\\n --referencePoint TSS \\\n --beforeRegionStartLength 3000 \\\n --afterRegionStartLength 3000 \\\n --blackListFileName $BLACKLIST \\\n --missingDataAsZero \\\n --numberOfProcessors $JOBS\n\necho \"Computing coverage over peak BEDs...\"\ncomputeMatrix reference-point \\\n  --regionsFileName $PEAKS \\\n  --scoreFileName $OUT/*.bw \\\n  --outFileName $OUT/peak.mat.gz \\\n  --referencePoint 'center' \\\n  --beforeRegionStartLength 3000 \\\n  --afterRegionStartLength 3000 \\\n  --blackListFileName $BLACKLIST \\\n  --missingDataAsZero \\\n  --numberOfProcessors $JOBS\n\necho \"Plotting heatmap of TSS enrichment...\"\nplotHeatmap -m $OUT/tss.mat.gz \\\n  -o $OUT/tss-heatmap.pdf \\\n  --colorMap \"viridis\" \\\n  --boxAroundHeatmaps 'no' \\\n  --samplesLabel \"Ctl 1\" \"Ctl 2\" \"Ctl 3\" \"Trt 1\" \"Trt 2\" \"Trt 3\" \\\n  --regionsLabel \"Protein Coding Genes\" \\\n  --yAxisLabel \"RPGC\" \\\n  --perGroup \\\n  --plotFileFormat \"pdf\" \\\n  --dpi 300 \\\n\necho \"Plotting heatmap of MACS peak enrichment...\"\nplotHeatmap -m $OUT/peak.mat.gz \\\n  -o $OUT/peak-heatmap.pdf \\\n  --colorMap \"viridis\" \\\n  --boxAroundHeatmaps 'no' \\\n  --samplesLabel \"Ctl 1\" \"Ctl 2\" \"Ctl 3\" \"Trt 1\" \"Trt 2\" \"Trt 3\" \\\n  --regionsLabel \"Consensus Peak Calls\" \\\n  --refPointLabel \"Center of Peak\" \\\n  --yAxisLabel \"RPGC\" \\\n  --perGroup \\\n  --plotFileFormat \"pdf\" \\\n  --dpi 300 \\\n\n\nGetting TSS coding region ranges\nFor ATAC-seq, I plot the enrichment over the TSS region for each sample as well as the coverage centered over the consensus peaks. To generate a BED file for protein coding gene regions (“coding.bed” above) in R:\n\n# Get mouse annotation GTF, for example\nurl <- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M37/gencode.vM37.annotation.gtf.gz\"\ngtf <- rtracklayer::import(url)\n\n# Extract the protein coding gene ranges only\ncoding <- gtf[gtf$gene_type == \"protein_coding\" & gtf$type == \"gene\", ]\n\n# rtracklayer complains if score is NA or non-numeric\ncoding$score <- 1L\n\n# Export to a BED file for use in deeptools\nrtracklayer::export.bed(coding, here(\"doc\", \"gencode.vM37.coding.bed\"))"
  },
  {
    "objectID": "atacseq-analysis.html#motif-analysis-with-meme",
    "href": "atacseq-analysis.html#motif-analysis-with-meme",
    "title": "ATAC-seq Analysis",
    "section": "Motif analysis with MEME",
    "text": "Motif analysis with MEME\nThe MEME suite can be used to perform motif analysis on peaks that have been determined to have differential abundance. You can use R to generate fasta files for these regions based on GRanges extracted from DA analysis. These fasta files can then be used as input to XSTREME, which performs de novo motif discovery, enrichment analysis, and comparisons with known motif databases.\nFor example, you can extract fasta regions like so (this uses hg38 as an example):\n\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(Biostrings))\nsuppressPackageStartupMessages(library(BSgenome.Hsapiens.UCSC.hg38))\nsuppressPackageStartupMessages(library(GenomicRanges))\n\n\n# Assuming you have a file from DA analysis that contains genomic coordinates\n# of peaks and annotations - for example from ChIPseeker::annotatePeak()\npeaks <- fread(\"annotated-peaks.tsv\")\npeaks <- makeGRangesFromDataFrame(peaks, keep.extra.columns = TRUE)\n\n# Extract promoter peaks -- this can be any filtering you're interested in \npromoter_peaks <- peaks[peaks$annotation %like% \"Promoter\"]\n\n# Extract and write out the hg38 sequences as fasta records\npmtr_seqs <- getSeq(BSgenome.Hsapiens.UCSC.hg38, promoter_peaks)\nwriteXStringSet(pmtr_seqs, filepath = \"promoter-peaks.fasta\", format = \"fasta\")\n\nThese fasta records can then be used in XSTREME analysis from the MEME suite. The meme formatted databases of known motifs can be downloaded from MEME’s website\n\n#!/usr/bin/env bash\n#\n# Run XSTREME motif analysis on promoter peak sequences\n#\n# -------------------------------------------------------------------------------------------------\nFA=/path/to/promoter-peaks.fasta\nDB=/path/to/HUMAN/HOCOMOCOv11_core_HUMAN_mono_meme_format.meme\nOUT=/path/to/xstreme\n\nxstreme --oc $OUT --p $FA --seed 123 --m $DB --no-pgc --dna"
  }
]